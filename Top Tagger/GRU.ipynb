{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(sys.path[0]),'lib'))\n",
    "\n",
    "from train import parse_config, get_features, print_model_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "from callbacks import all_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from ../train/train_config_gru.yml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Inputs': ['j1_ptrel',\n",
       "  'j1_etarot',\n",
       "  'j1_phirot',\n",
       "  'j1_erel',\n",
       "  'j1_deltaR',\n",
       "  'j1_pdgid',\n",
       "  'j_index'],\n",
       " 'Labels': ['j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_index'],\n",
       " 'KerasModel': 'gru_model',\n",
       " 'KerasModelRetrain': 'gru_model_constraint',\n",
       " 'KerasLoss': 'categorical_crossentropy',\n",
       " 'L1Reg': 0.0001,\n",
       " 'L1RegR': 0.001,\n",
       " 'NormalizeInputs': 1,\n",
       " 'InputType': 'Conv1D',\n",
       " 'MaxParticles': 20}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "Option = namedtuple(\"MyStruct\", \"inputModel inputFile tree config jsonModel\")\n",
    "\n",
    "options = Option(\n",
    "    inputModel = '../KERAS_gru_model_weights.h5',\n",
    "    inputFile = '../data/processed-pythia82-lhc13-all-pt1-50k-r1_h022_e0175_t220_nonu_withPars_truth_0.z',\n",
    "    tree = 't_allpar_new',\n",
    "    config = '../train/train_config_gru.yml',\n",
    "    jsonModel = '../KERAS_gru_model.json'\n",
    ")\n",
    "\n",
    "print(\"Loading configuration from\", options.config)\n",
    "config = open(options.config, 'r')\n",
    "yamlConfig =  yaml.load(config, Loader=yaml.FullLoader)\n",
    "\n",
    "yamlConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5131613,)\n",
      "('index', 'j_ptfrac', 'j_pt', 'j_eta', 'j_mass', 'j_tau1_b1', 'j_tau2_b1', 'j_tau3_b1', 'j_tau1_b2', 'j_tau2_b2', 'j_tau3_b2', 'j_tau32_b1', 'j_tau32_b2', 'j_zlogz', 'j_c1_b0', 'j_c1_b1', 'j_c1_b2', 'j_c2_b1', 'j_c2_b2', 'j_d2_b1', 'j_d2_b2', 'j_d2_a1_b1', 'j_d2_a1_b2', 'j_m2_b1', 'j_m2_b2', 'j_n2_b1', 'j_n2_b2', 'j_tau1_b1_mmdt', 'j_tau2_b1_mmdt', 'j_tau3_b1_mmdt', 'j_tau1_b2_mmdt', 'j_tau2_b2_mmdt', 'j_tau3_b2_mmdt', 'j_tau32_b1_mmdt', 'j_tau32_b2_mmdt', 'j_c1_b0_mmdt', 'j_c1_b1_mmdt', 'j_c1_b2_mmdt', 'j_c2_b1_mmdt', 'j_c2_b2_mmdt', 'j_d2_b1_mmdt', 'j_d2_b2_mmdt', 'j_d2_a1_b1_mmdt', 'j_d2_a1_b2_mmdt', 'j_m2_b1_mmdt', 'j_m2_b2_mmdt', 'j_n2_b1_mmdt', 'j_n2_b2_mmdt', 'j_mass_trim', 'j_mass_mmdt', 'j_mass_prun', 'j_mass_sdb2', 'j_mass_sdm1', 'j_multiplicity', 'j1_px', 'j1_py', 'j1_pz', 'j1_e', 'j1_pdgid', 'j1_erel', 'j1_pt', 'j1_ptrel', 'j1_eta', 'j1_etarel', 'j1_etarot', 'j1_phi', 'j1_phirel', 'j1_phirot', 'j1_deltaR', 'j1_costheta', 'j1_costhetarel', 'j1_e1mcosthetarel', 'j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_undef', 'j_index')\n",
      "(98769, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test, labels  = get_features(options, yamlConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1188: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:112: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "json_file = open(options.jsonModel, 'r')\n",
    "model = model_from_json(json_file.read())\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20, 6)             0         \n",
      "_________________________________________________________________\n",
      "gru_selu (GRU)               (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dense_relu (Dense)           (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "rnn_densef (Dense)           (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 2,145\n",
      "Trainable params: 2,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20, 6)             0         \n",
      "_________________________________________________________________\n",
      "gru_selu (GRU)               (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dense_relu (Dense)           (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "rnn_densef (Dense)           (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 2,145\n",
      "Trainable params: 2,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 59261 samples, validate on 19754 samples\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\callbacks.py:705: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\anrun\\anaconda3\\envs\\EPE_ML\\lib\\site-packages\\keras\\callbacks.py:708: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 2.9766 - acc: 0.2907\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00000: val_loss improved from inf to 1.93047, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00000: val_loss improved from inf to 1.93047, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00000: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00000: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 3s - loss: 2.9600 - acc: 0.2919 - val_loss: 1.9305 - val_acc: 0.3514\n",
      "Epoch 2/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.7853 - acc: 0.3726\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00001: val_loss improved from 1.93047 to 1.62377, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00001: val_loss improved from 1.93047 to 1.62377, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00001: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00001: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.7840 - acc: 0.3730 - val_loss: 1.6238 - val_acc: 0.3997\n",
      "Epoch 3/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.5945 - acc: 0.4125\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00002: val_loss improved from 1.62377 to 1.51596, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00002: val_loss improved from 1.62377 to 1.51596, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00002: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00002: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.5917 - acc: 0.4122 - val_loss: 1.5160 - val_acc: 0.4248\n",
      "Epoch 4/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.5172 - acc: 0.4279\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00003: val_loss improved from 1.51596 to 1.46258, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00003: val_loss improved from 1.51596 to 1.46258, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00003: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00003: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.5170 - acc: 0.4280 - val_loss: 1.4626 - val_acc: 0.4389\n",
      "Epoch 5/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.4707 - acc: 0.4378\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00004: val_loss improved from 1.46258 to 1.42714, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00004: val_loss improved from 1.46258 to 1.42714, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00004: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00004: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.4710 - acc: 0.4374 - val_loss: 1.4271 - val_acc: 0.4497\n",
      "Epoch 6/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.4405 - acc: 0.4458\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00005: val_loss improved from 1.42714 to 1.39925, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00005: val_loss improved from 1.42714 to 1.39925, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00005: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00005: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.4379 - acc: 0.4463 - val_loss: 1.3993 - val_acc: 0.4560\n",
      "Epoch 7/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.4116 - acc: 0.4537\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00006: val_loss improved from 1.39925 to 1.37617, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00006: val_loss improved from 1.39925 to 1.37617, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00006: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00006: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.4111 - acc: 0.4543 - val_loss: 1.3762 - val_acc: 0.4643\n",
      "Epoch 8/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.3867 - acc: 0.4631\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00007: val_loss improved from 1.37617 to 1.35654, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00007: val_loss improved from 1.37617 to 1.35654, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00007: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00007: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59261/59261 [==============================] - 1s - loss: 1.3875 - acc: 0.4624 - val_loss: 1.3565 - val_acc: 0.4742\n",
      "Epoch 9/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.3692 - acc: 0.4684\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00008: val_loss improved from 1.35654 to 1.33944, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00008: val_loss improved from 1.35654 to 1.33944, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00008: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00008: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3683 - acc: 0.4688 - val_loss: 1.3394 - val_acc: 0.4828\n",
      "Epoch 10/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.3518 - acc: 0.4760\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00009: val_loss improved from 1.33944 to 1.32481, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00009: val_loss improved from 1.33944 to 1.32481, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00009: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00009: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00009: saving model to .\\training_callbacks/KERAS_check_model_epoch09.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3515 - acc: 0.4760 - val_loss: 1.3248 - val_acc: 0.4877\n",
      "Epoch 11/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.3383 - acc: 0.4818\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00010: val_loss improved from 1.32481 to 1.31150, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00010: val_loss improved from 1.32481 to 1.31150, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00010: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00010: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3385 - acc: 0.4811 - val_loss: 1.3115 - val_acc: 0.4945\n",
      "Epoch 12/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.3273 - acc: 0.4873\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00011: val_loss improved from 1.31150 to 1.29972, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00011: val_loss improved from 1.31150 to 1.29972, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00011: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00011: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3273 - acc: 0.4876 - val_loss: 1.2997 - val_acc: 0.5008\n",
      "Epoch 13/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.3146 - acc: 0.4927\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00012: val_loss improved from 1.29972 to 1.28914, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00012: val_loss improved from 1.29972 to 1.28914, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00012: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00012: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3148 - acc: 0.4929 - val_loss: 1.2891 - val_acc: 0.5077\n",
      "Epoch 14/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.3014 - acc: 0.4993\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00013: val_loss improved from 1.28914 to 1.27919, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00013: val_loss improved from 1.28914 to 1.27919, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00013: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00013: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.3011 - acc: 0.4992 - val_loss: 1.2792 - val_acc: 0.5128\n",
      "Epoch 15/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2960 - acc: 0.5001\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00014: val_loss improved from 1.27919 to 1.27019, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00014: val_loss improved from 1.27919 to 1.27019, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00014: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00014: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2944 - acc: 0.5009 - val_loss: 1.2702 - val_acc: 0.5175\n",
      "Epoch 16/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2839 - acc: 0.5073\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00015: val_loss improved from 1.27019 to 1.26188, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00015: val_loss improved from 1.27019 to 1.26188, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00015: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00015: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2834 - acc: 0.5075 - val_loss: 1.2619 - val_acc: 0.5221\n",
      "Epoch 17/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2747 - acc: 0.5100\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00016: val_loss improved from 1.26188 to 1.25411, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00016: val_loss improved from 1.26188 to 1.25411, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00016: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00016: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2762 - acc: 0.5097 - val_loss: 1.2541 - val_acc: 0.5250\n",
      "Epoch 18/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2677 - acc: 0.5144\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00017: val_loss improved from 1.25411 to 1.24676, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00017: val_loss improved from 1.25411 to 1.24676, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00017: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00017: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2676 - acc: 0.5144 - val_loss: 1.2468 - val_acc: 0.5291\n",
      "Epoch 19/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2630 - acc: 0.5160\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00018: val_loss improved from 1.24676 to 1.23981, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00018: val_loss improved from 1.24676 to 1.23981, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00018: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00018: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2634 - acc: 0.5166 - val_loss: 1.2398 - val_acc: 0.5323\n",
      "Epoch 20/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2551 - acc: 0.5202\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00019: val_loss improved from 1.23981 to 1.23350, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00019: val_loss improved from 1.23981 to 1.23350, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00019: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00019: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00019: saving model to .\\training_callbacks/KERAS_check_model_epoch19.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59261/59261 [==============================] - 1s - loss: 1.2556 - acc: 0.5202 - val_loss: 1.2335 - val_acc: 0.5358\n",
      "Epoch 21/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2490 - acc: 0.5227\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00020: val_loss improved from 1.23350 to 1.22727, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00020: val_loss improved from 1.23350 to 1.22727, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00020: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00020: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2490 - acc: 0.5228 - val_loss: 1.2273 - val_acc: 0.5387\n",
      "Epoch 22/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2431 - acc: 0.5262\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00021: val_loss improved from 1.22727 to 1.22147, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00021: val_loss improved from 1.22727 to 1.22147, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00021: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00021: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2419 - acc: 0.5265 - val_loss: 1.2215 - val_acc: 0.5409\n",
      "Epoch 23/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2363 - acc: 0.5294\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00022: val_loss improved from 1.22147 to 1.21595, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00022: val_loss improved from 1.22147 to 1.21595, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00022: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00022: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2374 - acc: 0.5284 - val_loss: 1.2159 - val_acc: 0.5442\n",
      "Epoch 24/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2313 - acc: 0.5322\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00023: val_loss improved from 1.21595 to 1.21096, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00023: val_loss improved from 1.21595 to 1.21096, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00023: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00023: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2315 - acc: 0.5322 - val_loss: 1.2110 - val_acc: 0.5461\n",
      "Epoch 25/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.2273 - acc: 0.5342\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00024: val_loss improved from 1.21096 to 1.20609, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00024: val_loss improved from 1.21096 to 1.20609, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00024: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00024: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2270 - acc: 0.5343 - val_loss: 1.2061 - val_acc: 0.5475\n",
      "Epoch 26/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2236 - acc: 0.5352\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00025: val_loss improved from 1.20609 to 1.20149, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00025: val_loss improved from 1.20609 to 1.20149, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00025: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00025: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2228 - acc: 0.5354 - val_loss: 1.2015 - val_acc: 0.5504\n",
      "Epoch 27/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.2184 - acc: 0.5392\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00026: val_loss improved from 1.20149 to 1.19678, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00026: val_loss improved from 1.20149 to 1.19678, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00026: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00026: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2189 - acc: 0.5391 - val_loss: 1.1968 - val_acc: 0.5523\n",
      "Epoch 28/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2147 - acc: 0.5417\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00027: val_loss improved from 1.19678 to 1.19281, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00027: val_loss improved from 1.19678 to 1.19281, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00027: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00027: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2142 - acc: 0.5417 - val_loss: 1.1928 - val_acc: 0.5546\n",
      "Epoch 29/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2079 - acc: 0.5437\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00028: val_loss improved from 1.19281 to 1.18831, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00028: val_loss improved from 1.19281 to 1.18831, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00028: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00028: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2083 - acc: 0.5440 - val_loss: 1.1883 - val_acc: 0.5572\n",
      "Epoch 30/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2055 - acc: 0.5454\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00029: val_loss improved from 1.18831 to 1.18428, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00029: val_loss improved from 1.18831 to 1.18428, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00029: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00029: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00029: saving model to .\\training_callbacks/KERAS_check_model_epoch29.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2048 - acc: 0.5455 - val_loss: 1.1843 - val_acc: 0.5598\n",
      "Epoch 31/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.2003 - acc: 0.5471\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00030: val_loss improved from 1.18428 to 1.18042, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00030: val_loss improved from 1.18428 to 1.18042, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00030: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00030: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.2009 - acc: 0.5464 - val_loss: 1.1804 - val_acc: 0.5612\n",
      "Epoch 32/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1981 - acc: 0.5479\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00031: val_loss improved from 1.18042 to 1.17625, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00031: val_loss improved from 1.18042 to 1.17625, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00031: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00031: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1977 - acc: 0.5481 - val_loss: 1.1763 - val_acc: 0.5640\n",
      "Epoch 33/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1946 - acc: 0.5491\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00032: val_loss improved from 1.17625 to 1.17258, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00032: val_loss improved from 1.17625 to 1.17258, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00032: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00032: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1942 - acc: 0.5494 - val_loss: 1.1726 - val_acc: 0.5662\n",
      "Epoch 34/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1891 - acc: 0.5521\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00033: val_loss improved from 1.17258 to 1.16866, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00033: val_loss improved from 1.17258 to 1.16866, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00033: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00033: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1896 - acc: 0.5522 - val_loss: 1.1687 - val_acc: 0.5664\n",
      "Epoch 35/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1875 - acc: 0.5548\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00034: val_loss improved from 1.16866 to 1.16501, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00034: val_loss improved from 1.16866 to 1.16501, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00034: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00034: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1869 - acc: 0.5549 - val_loss: 1.1650 - val_acc: 0.5682\n",
      "Epoch 36/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1820 - acc: 0.5561\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00035: val_loss improved from 1.16501 to 1.16154, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00035: val_loss improved from 1.16501 to 1.16154, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00035: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00035: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1823 - acc: 0.5555 - val_loss: 1.1615 - val_acc: 0.5702\n",
      "Epoch 37/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1795 - acc: 0.5577\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00036: val_loss improved from 1.16154 to 1.15813, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00036: val_loss improved from 1.16154 to 1.15813, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00036: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00036: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1810 - acc: 0.5571 - val_loss: 1.1581 - val_acc: 0.5713\n",
      "Epoch 38/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1768 - acc: 0.5592- ETA: 0s - loss: 1.1819 \n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00037: val_loss improved from 1.15813 to 1.15446, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00037: val_loss improved from 1.15813 to 1.15446, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00037: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00037: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1757 - acc: 0.5599 - val_loss: 1.1545 - val_acc: 0.5737\n",
      "Epoch 39/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.1733 - acc: 0.5603\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00038: val_loss improved from 1.15446 to 1.15095, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00038: val_loss improved from 1.15446 to 1.15095, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00038: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00038: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1730 - acc: 0.5606 - val_loss: 1.1509 - val_acc: 0.5746\n",
      "Epoch 40/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1692 - acc: 0.5611\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00039: val_loss improved from 1.15095 to 1.14758, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00039: val_loss improved from 1.15095 to 1.14758, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00039: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00039: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00039: saving model to .\\training_callbacks/KERAS_check_model_epoch39.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1678 - acc: 0.5618 - val_loss: 1.1476 - val_acc: 0.5763\n",
      "Epoch 41/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1657 - acc: 0.5665\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00040: val_loss improved from 1.14758 to 1.14471, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00040: val_loss improved from 1.14758 to 1.14471, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00040: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00040: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1658 - acc: 0.5665 - val_loss: 1.1447 - val_acc: 0.5784\n",
      "Epoch 42/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1640 - acc: 0.5646\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00041: val_loss improved from 1.14471 to 1.14107, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00041: val_loss improved from 1.14471 to 1.14107, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00041: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00041: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1634 - acc: 0.5649 - val_loss: 1.1411 - val_acc: 0.5793\n",
      "Epoch 43/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1594 - acc: 0.5662\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00042: val_loss improved from 1.14107 to 1.13831, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00042: val_loss improved from 1.14107 to 1.13831, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00042: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00042: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59261/59261 [==============================] - 1s - loss: 1.1595 - acc: 0.5664 - val_loss: 1.1383 - val_acc: 0.5787\n",
      "Epoch 44/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1596 - acc: 0.5660\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00043: val_loss improved from 1.13831 to 1.13493, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00043: val_loss improved from 1.13831 to 1.13493, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00043: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00043: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1581 - acc: 0.5664 - val_loss: 1.1349 - val_acc: 0.5811\n",
      "Epoch 45/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1549 - acc: 0.5684\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00044: val_loss improved from 1.13493 to 1.13241, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00044: val_loss improved from 1.13493 to 1.13241, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00044: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00044: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1546 - acc: 0.5684 - val_loss: 1.1324 - val_acc: 0.5817\n",
      "Epoch 46/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1525 - acc: 0.5684\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00045: val_loss improved from 1.13241 to 1.12965, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00045: val_loss improved from 1.13241 to 1.12965, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00045: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00045: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1518 - acc: 0.5684 - val_loss: 1.1297 - val_acc: 0.5817\n",
      "Epoch 47/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1467 - acc: 0.5735\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00046: val_loss improved from 1.12965 to 1.12639, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00046: val_loss improved from 1.12965 to 1.12639, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00046: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00046: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1477 - acc: 0.5732 - val_loss: 1.1264 - val_acc: 0.5845\n",
      "Epoch 48/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1475 - acc: 0.5717- ETA: 0s - loss: 1.1490 \n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00047: val_loss improved from 1.12639 to 1.12411, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00047: val_loss improved from 1.12639 to 1.12411, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00047: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00047: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1470 - acc: 0.5721 - val_loss: 1.1241 - val_acc: 0.5850\n",
      "Epoch 49/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1436 - acc: 0.5744\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00048: val_loss improved from 1.12411 to 1.12108, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00048: val_loss improved from 1.12411 to 1.12108, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00048: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00048: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1439 - acc: 0.5747 - val_loss: 1.1211 - val_acc: 0.5883\n",
      "Epoch 50/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1409 - acc: 0.5757\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00049: val_loss improved from 1.12108 to 1.11859, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00049: val_loss improved from 1.12108 to 1.11859, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00049: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00049: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00049: saving model to .\\training_callbacks/KERAS_check_model_epoch49.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1404 - acc: 0.5760 - val_loss: 1.1186 - val_acc: 0.5901\n",
      "Epoch 51/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1396 - acc: 0.5756\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00050: val_loss improved from 1.11859 to 1.11634, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00050: val_loss improved from 1.11859 to 1.11634, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00050: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00050: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1389 - acc: 0.5759 - val_loss: 1.1163 - val_acc: 0.5899\n",
      "Epoch 52/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1345 - acc: 0.5785\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00051: val_loss improved from 1.11634 to 1.11361, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00051: val_loss improved from 1.11634 to 1.11361, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00051: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00051: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1350 - acc: 0.5780 - val_loss: 1.1136 - val_acc: 0.5916\n",
      "Epoch 53/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1341 - acc: 0.5797\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00052: val_loss improved from 1.11361 to 1.11124, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00052: val_loss improved from 1.11361 to 1.11124, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00052: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00052: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1333 - acc: 0.5805 - val_loss: 1.1112 - val_acc: 0.5922\n",
      "Epoch 54/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1330 - acc: 0.5812\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00053: val_loss improved from 1.11124 to 1.11007, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00053: val_loss improved from 1.11124 to 1.11007, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00053: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00053: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1317 - acc: 0.5820 - val_loss: 1.1101 - val_acc: 0.5930\n",
      "Epoch 55/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.1300 - acc: 0.5820\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00054: val_loss improved from 1.11007 to 1.10744, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00054: val_loss improved from 1.11007 to 1.10744, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00054: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00054: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59261/59261 [==============================] - 1s - loss: 1.1298 - acc: 0.5816 - val_loss: 1.1074 - val_acc: 0.5953\n",
      "Epoch 56/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1292 - acc: 0.5834\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00055: val_loss improved from 1.10744 to 1.10505, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00055: val_loss improved from 1.10744 to 1.10505, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00055: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00055: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1280 - acc: 0.5836 - val_loss: 1.1050 - val_acc: 0.5957\n",
      "Epoch 57/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1247 - acc: 0.5864\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00056: val_loss improved from 1.10505 to 1.10255, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00056: val_loss improved from 1.10505 to 1.10255, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00056: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00056: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1257 - acc: 0.5852 - val_loss: 1.1026 - val_acc: 0.5962\n",
      "Epoch 58/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.1225 - acc: 0.5874\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00057: val_loss improved from 1.10255 to 1.10040, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00057: val_loss improved from 1.10255 to 1.10040, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00057: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00057: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1230 - acc: 0.5873 - val_loss: 1.1004 - val_acc: 0.5981\n",
      "Epoch 59/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1210 - acc: 0.5888\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00058: val_loss improved from 1.10040 to 1.09851, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00058: val_loss improved from 1.10040 to 1.09851, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00058: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00058: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1210 - acc: 0.5888 - val_loss: 1.0985 - val_acc: 0.5990\n",
      "Epoch 60/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1195 - acc: 0.5888\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00059: val_loss improved from 1.09851 to 1.09640, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00059: val_loss improved from 1.09851 to 1.09640, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00059: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00059: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00059: saving model to .\\training_callbacks/KERAS_check_model_epoch59.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1198 - acc: 0.5890 - val_loss: 1.0964 - val_acc: 0.6001\n",
      "Epoch 61/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1166 - acc: 0.5901\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00060: val_loss improved from 1.09640 to 1.09445, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00060: val_loss improved from 1.09640 to 1.09445, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00060: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00060: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1182 - acc: 0.5899 - val_loss: 1.0945 - val_acc: 0.6019\n",
      "Epoch 62/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1135 - acc: 0.5925\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00061: val_loss improved from 1.09445 to 1.09252, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00061: val_loss improved from 1.09445 to 1.09252, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00061: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00061: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1146 - acc: 0.5919 - val_loss: 1.0925 - val_acc: 0.6029\n",
      "Epoch 63/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1148 - acc: 0.5913\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00062: val_loss improved from 1.09252 to 1.09053, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00062: val_loss improved from 1.09252 to 1.09053, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00062: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00062: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1138 - acc: 0.5917 - val_loss: 1.0905 - val_acc: 0.6043\n",
      "Epoch 64/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1110 - acc: 0.5931\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00063: val_loss improved from 1.09053 to 1.08876, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00063: val_loss improved from 1.09053 to 1.08876, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00063: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00063: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1117 - acc: 0.5930 - val_loss: 1.0888 - val_acc: 0.6052\n",
      "Epoch 65/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1086 - acc: 0.5945\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00064: val_loss improved from 1.08876 to 1.08693, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00064: val_loss improved from 1.08876 to 1.08693, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00064: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00064: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1090 - acc: 0.5947 - val_loss: 1.0869 - val_acc: 0.6059\n",
      "Epoch 66/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.1076 - acc: 0.5978\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00065: val_loss improved from 1.08693 to 1.08520, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00065: val_loss improved from 1.08693 to 1.08520, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00065: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00065: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1075 - acc: 0.5977 - val_loss: 1.0852 - val_acc: 0.6073\n",
      "Epoch 67/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1046 - acc: 0.5972\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00066: val_loss improved from 1.08520 to 1.08351, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00066: val_loss improved from 1.08520 to 1.08351, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00066: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00066: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59261/59261 [==============================] - 1s - loss: 1.1060 - acc: 0.5967 - val_loss: 1.0835 - val_acc: 0.6074\n",
      "Epoch 68/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1074 - acc: 0.5971\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00067: val_loss improved from 1.08351 to 1.08124, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00067: val_loss improved from 1.08351 to 1.08124, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00067: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00067: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1055 - acc: 0.5982 - val_loss: 1.0812 - val_acc: 0.6094\n",
      "Epoch 69/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.1012 - acc: 0.5986\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00068: val_loss improved from 1.08124 to 1.07963, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00068: val_loss improved from 1.08124 to 1.07963, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00068: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00068: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1014 - acc: 0.5988 - val_loss: 1.0796 - val_acc: 0.6094\n",
      "Epoch 70/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.6001\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00069: val_loss improved from 1.07963 to 1.07778, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00069: val_loss improved from 1.07963 to 1.07778, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00069: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00069: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00069: saving model to .\\training_callbacks/KERAS_check_model_epoch69.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.1018 - acc: 0.5999 - val_loss: 1.0778 - val_acc: 0.6108\n",
      "Epoch 71/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0981 - acc: 0.6011\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00070: val_loss improved from 1.07778 to 1.07609, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00070: val_loss improved from 1.07778 to 1.07609, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00070: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00070: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0989 - acc: 0.6009 - val_loss: 1.0761 - val_acc: 0.6114\n",
      "Epoch 72/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.0978 - acc: 0.6011\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00071: val_loss improved from 1.07609 to 1.07445, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00071: val_loss improved from 1.07609 to 1.07445, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00071: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00071: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0977 - acc: 0.6012 - val_loss: 1.0745 - val_acc: 0.6135\n",
      "Epoch 73/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0950 - acc: 0.6035\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00072: val_loss improved from 1.07445 to 1.07281, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00072: val_loss improved from 1.07445 to 1.07281, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00072: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00072: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0945 - acc: 0.6041 - val_loss: 1.0728 - val_acc: 0.6141\n",
      "Epoch 74/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0945 - acc: 0.6038\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 00073: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00073: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0929 - acc: 0.6045 - val_loss: 1.0733 - val_acc: 0.6140\n",
      "Epoch 75/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0929 - acc: 0.6043\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00074: val_loss improved from 1.07281 to 1.06990, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00074: val_loss improved from 1.07281 to 1.06990, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00074: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00074: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0932 - acc: 0.6042 - val_loss: 1.0699 - val_acc: 0.6173\n",
      "Epoch 76/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0914 - acc: 0.6045\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00075: val_loss improved from 1.06990 to 1.06768, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00075: val_loss improved from 1.06990 to 1.06768, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00075: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00075: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0918 - acc: 0.6042 - val_loss: 1.0677 - val_acc: 0.6182\n",
      "Epoch 77/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0886 - acc: 0.6066\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 00076: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00076: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0884 - acc: 0.6067 - val_loss: 1.0680 - val_acc: 0.6171\n",
      "Epoch 78/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0878 - acc: 0.6084\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00077: val_loss improved from 1.06768 to 1.06473, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00077: val_loss improved from 1.06768 to 1.06473, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00077: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00077: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0879 - acc: 0.6082 - val_loss: 1.0647 - val_acc: 0.6204\n",
      "Epoch 79/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.0882 - acc: 0.6080\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00078: val_loss improved from 1.06473 to 1.06298, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00078: val_loss improved from 1.06473 to 1.06298, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00078: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00078: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59261/59261 [==============================] - 1s - loss: 1.0878 - acc: 0.6083 - val_loss: 1.0630 - val_acc: 0.6209\n",
      "Epoch 80/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.0856 - acc: 0.6084\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00079: val_loss improved from 1.06298 to 1.06181, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00079: val_loss improved from 1.06298 to 1.06181, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00079: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00079: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00079: saving model to .\\training_callbacks/KERAS_check_model_epoch79.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0852 - acc: 0.6085 - val_loss: 1.0618 - val_acc: 0.6209\n",
      "Epoch 81/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0834 - acc: 0.6112\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00080: val_loss improved from 1.06181 to 1.06010, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00080: val_loss improved from 1.06181 to 1.06010, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00080: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00080: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0833 - acc: 0.6114 - val_loss: 1.0601 - val_acc: 0.6238\n",
      "Epoch 82/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0838 - acc: 0.6123\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00081: val_loss improved from 1.06010 to 1.05844, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00081: val_loss improved from 1.06010 to 1.05844, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00081: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00081: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0827 - acc: 0.6128 - val_loss: 1.0584 - val_acc: 0.6239\n",
      "Epoch 83/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0796 - acc: 0.6130\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00082: val_loss improved from 1.05844 to 1.05651, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00082: val_loss improved from 1.05844 to 1.05651, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00082: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00082: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0796 - acc: 0.6129 - val_loss: 1.0565 - val_acc: 0.6256\n",
      "Epoch 84/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0770 - acc: 0.6160\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00083: val_loss improved from 1.05651 to 1.05583, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00083: val_loss improved from 1.05651 to 1.05583, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00083: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00083: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0777 - acc: 0.6157 - val_loss: 1.0558 - val_acc: 0.6248\n",
      "Epoch 85/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0773 - acc: 0.6156\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00084: val_loss improved from 1.05583 to 1.05335, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00084: val_loss improved from 1.05583 to 1.05335, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00084: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00084: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0764 - acc: 0.6165 - val_loss: 1.0534 - val_acc: 0.6279\n",
      "Epoch 86/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.0770 - acc: 0.6143\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00085: val_loss improved from 1.05335 to 1.05199, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00085: val_loss improved from 1.05335 to 1.05199, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00085: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00085: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0753 - acc: 0.6154 - val_loss: 1.0520 - val_acc: 0.6288\n",
      "Epoch 87/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0737 - acc: 0.6169\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00086: val_loss improved from 1.05199 to 1.05048, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00086: val_loss improved from 1.05199 to 1.05048, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00086: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00086: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0744 - acc: 0.6167 - val_loss: 1.0505 - val_acc: 0.6285\n",
      "Epoch 88/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0738 - acc: 0.6167\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00087: val_loss improved from 1.05048 to 1.04885, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00087: val_loss improved from 1.05048 to 1.04885, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00087: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00087: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0736 - acc: 0.6167 - val_loss: 1.0488 - val_acc: 0.6307\n",
      "Epoch 89/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0724 - acc: 0.6192\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00088: val_loss improved from 1.04885 to 1.04783, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00088: val_loss improved from 1.04885 to 1.04783, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00088: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00088: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0726 - acc: 0.6190 - val_loss: 1.0478 - val_acc: 0.6319\n",
      "Epoch 90/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0696 - acc: 0.6222\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00089: val_loss improved from 1.04783 to 1.04573, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00089: val_loss improved from 1.04783 to 1.04573, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00089: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00089: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00089: saving model to .\\training_callbacks/KERAS_check_model_epoch89.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0689 - acc: 0.6222 - val_loss: 1.0457 - val_acc: 0.6332\n",
      "Epoch 91/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0680 - acc: 0.6220\n",
      "***callbacks***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00090: val_loss improved from 1.04573 to 1.04373, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00090: val_loss improved from 1.04573 to 1.04373, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00090: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00090: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0677 - acc: 0.6219 - val_loss: 1.0437 - val_acc: 0.6344\n",
      "Epoch 92/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0679 - acc: 0.6228\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00091: val_loss improved from 1.04373 to 1.04234, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00091: val_loss improved from 1.04373 to 1.04234, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00091: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00091: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0664 - acc: 0.6233 - val_loss: 1.0423 - val_acc: 0.6352\n",
      "Epoch 93/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0672 - acc: 0.6241\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00092: val_loss improved from 1.04234 to 1.04069, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00092: val_loss improved from 1.04234 to 1.04069, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00092: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00092: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0668 - acc: 0.6241 - val_loss: 1.0407 - val_acc: 0.6363\n",
      "Epoch 94/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0651 - acc: 0.6248\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00093: val_loss improved from 1.04069 to 1.03897, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00093: val_loss improved from 1.04069 to 1.03897, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00093: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00093: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0643 - acc: 0.6254 - val_loss: 1.0390 - val_acc: 0.6371\n",
      "Epoch 95/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0620 - acc: 0.6272\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00094: val_loss improved from 1.03897 to 1.03834, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00094: val_loss improved from 1.03897 to 1.03834, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00094: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00094: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0622 - acc: 0.6271 - val_loss: 1.0383 - val_acc: 0.6386\n",
      "Epoch 96/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.0601 - acc: 0.6270\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00095: val_loss improved from 1.03834 to 1.03607, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00095: val_loss improved from 1.03834 to 1.03607, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00095: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00095: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0610 - acc: 0.6268 - val_loss: 1.0361 - val_acc: 0.6391\n",
      "Epoch 97/100\n",
      "57344/59261 [============================>.] - ETA: 0s - loss: 1.0589 - acc: 0.6262\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00096: val_loss improved from 1.03607 to 1.03448, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00096: val_loss improved from 1.03607 to 1.03448, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00096: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00096: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0585 - acc: 0.6268 - val_loss: 1.0345 - val_acc: 0.6410\n",
      "Epoch 98/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0548 - acc: 0.6315\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00097: val_loss improved from 1.03448 to 1.03261, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00097: val_loss improved from 1.03448 to 1.03261, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00097: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00097: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0559 - acc: 0.6309 - val_loss: 1.0326 - val_acc: 0.6417\n",
      "Epoch 99/100\n",
      "56320/59261 [===========================>..] - ETA: 0s - loss: 1.0544 - acc: 0.6297\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00098: val_loss improved from 1.03261 to 1.03076, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00098: val_loss improved from 1.03261 to 1.03076, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00098: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00098: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0545 - acc: 0.6300 - val_loss: 1.0308 - val_acc: 0.6428\n",
      "Epoch 100/100\n",
      "58368/59261 [============================>.] - ETA: 0s - loss: 1.0548 - acc: 0.6310\n",
      "***callbacks***\n",
      "saving losses to .\\training_callbacks\\losses.log\n",
      "Epoch 00099: val_loss improved from 1.03076 to 1.02903, saving model to .\\training_callbacks/KERAS_check_best_model.h5\n",
      "Epoch 00099: val_loss improved from 1.03076 to 1.02903, saving model to .\\training_callbacks/KERAS_check_best_model_weights.h5\n",
      "Epoch 00099: saving model to .\\training_callbacks/KERAS_check_model_last.h5\n",
      "Epoch 00099: saving model to .\\training_callbacks/KERAS_check_model_last_weights.h5\n",
      "Epoch 00099: saving model to .\\training_callbacks/KERAS_check_model_epoch99.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "59261/59261 [==============================] - 1s - loss: 1.0549 - acc: 0.6310 - val_loss: 1.0290 - val_acc: 0.6442\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(options.inputModel)\n",
    "model.summary()\n",
    "startlearningrate=0.0001\n",
    "adam = Adam(lr=startlearningrate)\n",
    "model.compile(optimizer=adam, loss=[yamlConfig['KerasLoss']], metrics=['accuracy'])\n",
    "\n",
    "callbacks=all_callbacks(stop_patience=1000, \n",
    "                        lr_factor=0.5,\n",
    "                        lr_patience=10,\n",
    "                        lr_epsilon=0.000001, \n",
    "                        lr_cooldown=2, \n",
    "                        lr_minimum=0.0000001,\n",
    "                        outputDir=os.curdir + '\\\\training_callbacks')\n",
    "\n",
    "history = model.fit(X_train_val, y_train_val, batch_size = 1024, epochs = 100,\n",
    "                validation_split = 0.25, shuffle = True, callbacks = callbacks.callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRoc(features_val, labels, labels_val, model):\n",
    "    print('in makeRoc()')\n",
    "    if 'j_index' in labels: labels.remove('j_index')\n",
    "\n",
    "    predict_test = model.predict(features_val)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    auc1 = {}\n",
    "    \n",
    "    plt.figure()       \n",
    "    for i, label in enumerate(labels):\n",
    "        df[label] = labels_val[:,i]\n",
    "        df[label + '_pred'] = predict_test[:,i]\n",
    "        \n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "            \n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, AUC = %.1f%%'%(label.replace('j_',''),auc1[label]*100.))\n",
    "    plt.semilogy()\n",
    "    plt.xlabel(\"Signal Efficiency\")\n",
    "    plt.ylabel(\"Background Efficiency\")\n",
    "    plt.ylim(0.001,1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.figtext(0.25, 0.90,'hls4ml',fontweight='bold', wrap=True, horizontalalignment='right', fontsize=14)\n",
    "    #plt.figtext(0.35, 0.90,'preliminary', style='italic', wrap=True, horizontalalignment='center', fontsize=14) \n",
    "    plt.savefig(\"mygraph.png\")\n",
    "#     plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "#     plt.savefig('%s/ROC.pdf' %(outputDir))\n",
    "    return predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot ROC curve\n",
      "in makeRoc()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Plot ROC curve\")\n",
    "y_predict = makeRoc(X_test, labels, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.savefig('Learning_curve.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAHwCAYAAADjOch3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5jVZb3//+d7rVnIUUBARQEBQ0TOSKgbE8lS8ZSHMksrtDTtYLbLrbV3ZvXrW+2s3LUtt5ZZZpl5KEtNszzmIYEABTyhKAgqIOeDzOH+/bEW08wwA6w1azEDPh/XNVezPp/781n3WsN1+ep9f+77jpQSkiRJah8ybd0BSZIk/YvhTJIkqR0xnEmSJLUjhjNJkqR2xHAmSZLUjhjOJEmS2hHDmaSKiYiBEZEiomo72k6NiEd2RL/UPP8GUvtgOJMEQEQsiIhNEdG7yfGZhYA1sG16VlzI21VExJERURcRa5v8HNbWfZNUWYYzSQ29BHxo84uIGAl0arvuvD1sJXQuTil1bfLz2A7tnKQdznAmqaEbgI82eP0x4JcNG0RE94j4ZUQsjYiXI+K/IiJTOJeNiCsiYllEvAgc38y1P4uIJRHxakT8fxGRbU2HI2KfiLgjIt6MiBci4twG5yZExLSIWB0Rr0fE9wvHO0bEryJieUSsjIgnI2KvFu4/LCIeKLSbExEnFY4fGhGvNex/RJwSEbMLv2ci4tKImF94n5sjYo/Cuc2VwI9HxCvA30r43A9ExLci4h8RsSoi/rD5/oXzJxX6u7LQdliDc/0j4rbC33B5RPxvk3tfERErIuKliJjS4PjUiHgxItYUzp1ZbL8lbZvhTFJDjwO7FwJJFvgg8KsmbX4EdAcGA5PIh7mzC+fOBU4AxgLjgfc3ufYXQA3wjkKbo4FPtLLPvwEWAfsU3u//RcRRhXP/A/xPSml3YH/g5sLxjxU+Q3+gF3A+sKHpjSMiB/wRuBfYE/gscGNEDE0pPQ6sA97d4JIPA78u/H4hcDL572gfYAVwVZO3mAQMA44p5YOT/+7PKdy/Bvhhod8HkP9eLgL6AHcBf4yIDoW/65+Al4GBwL7ATQ3ueQjwLNAb+G/gZ5HXpXD/KSmlbsC/ATNL7LekrTCcSWpqc/XsvcAzwKubTzQIbF9KKa1JKS0Avgd8pNDkdODKlNLClNKbwLcaXLsXMAW4KKW0LqX0BvAD4IxSOxoR/YHDgUtSShtTSjOBnzboTzXwjojonVJaWwhUm4/3At6RUqpNKU1PKa1u5i0OBboC304pbUop/Y18sNk89Pubzb9HRDfguMIxgE8C/5lSWpRSegu4HHh/kyHMywvfxRbBsGCfQuWr4U+XBudvSCk9nVJaB3wFOL3B3+jOlNJfUkrVwBXkh6f/DZhAPsxdXHjvjSmlhpMAXk4pXZtSqiUfpvsCm6uKdcCIiOiUUlqSUprTQr8ltYLhTFJTN5CvAE2lyZAm+WpKB/JVl81eJl99gfx/9Bc2ObfZfkAOWLI5aAD/R74iVap9gDdTSmta6M/HgQOAZwpDlycUjt8A3APcFBGLI+K/C1Wy5u6/MKVU18L9fw2cGhG7AacCM1JKmz/zfsDtDT7rPKCWfwUdaPxdNWdxSqlHk591LVz/Mvnvt3eh3/XffaH/Cwv97k8+gNW08J6vNbhufeHXroX3/SD5KuOSiLgzIg7cRv8llcBwJqmRQrh4iXwV6LYmp5eRrzrt1+DYAP5VXVtC/j/+Dc9tthB4C+jdIGjsnlIa3oruLgb2KFSttuhPSun5lNKHyAfA7wC3RESXlFJ1SulrKaWDyFeTTqDxs3YN799/8zN1zdx/LvkQNIXGQ5qbP++UJsGqY0rp1QZtUukfHdjyu64m/zdaTIO/UUREoe2rhX4N2MokhBallO5JKb2XfDXtGeDa0rsuqSWGM0nN+Tjw7iZVGgpDXTcD34yIbhGxH/Dv/Ou5tJuBCyOiX0T0BC5tcO0S8s9ufS8idi88ML9/REwqol+7FR7m7xgRHcmHjUeBbxWOjSr0/UaAiDgrIvoUKkcrC/eojYjJETGyMAS4mnyoqW3m/Z4g/1zZf0RELiKOBE6k8TNavyb/fNkRwO8aHL+68D3tV+hLn4h4XxGfdXucFREHRURn4OvALQ3+RsdHxFGFiuAXyAfjR4F/kA/R346ILoXvbeK23igi9ipMMuhSuNdamv/OJLWS4UzSFlJK81NK01o4/VnygeVF4BHy4eS6wrlryQ8XzgJmsGXl7aPkh0Xnkn9A/hbyVZjttZb8g/ubf95N/pmvgeSrRbcDX00p/aXQ/lhgTkSsJT854IyU0kZg78J7ryY/3PggW058IKW0CTiJfGVsGfBj4KMppWcaNPsNcCTwt5TSsgbH/we4A7g3ItaQn2xxSBGfFfLPnDVd5+y0BudvAK4nPxTZkXxIJKX0LHAW+ckby8gHyhMLz83VFl6/A3iF/GSKD25HXzLkQ95i4E3ykxk+VeTnkbQdIqXWVtUlSTtaRDwA/Cql9NO27ouk8rJyJkmS1I4YziRJktoRhzUlSZLaEStnkiRJ7YjhTJIkqR0pehHC9qx3795p4MCBbd0NSZKkbZo+ffqylFKfpsd3qXA2cOBApk1raWkmSZKk9iMiXm7uuMOakiRJ7YjhTJIkqR0xnEmSJLUju9QzZ5IklUt1dTWLFi1i48aNbd0V7eQ6duxIv379yOVy29XecCZJUjMWLVpEt27dGDhwIBHR1t3RTiqlxPLly1m0aBGDBg3armsc1pQkqRkbN26kV69eBjO1SkTQq1evoiqwFQtnEdExIv4REbMiYk5EfK2ZNhERP4yIFyJidkSMa3Du2Ih4tnDu0kr1U5KklhjMVA7F/juqZOXsLeDdKaXRwBjg2Ig4tEmbKcCQws95wE8AIiILXFU4fxDwoYg4qIJ9lSSpXVm5ciU//vGPS7r2uOOOY+XKlVttc9lll3HfffeVdP+2NHXqVG655Zbtbn/55ZdzxRVXVLBH5VexcJby1hZe5go/TXdZfx/wy0Lbx4EeEdEXmAC8kFJ6MaW0Cbip0FaSpLeFrYWz2trarV5711130aNHj622+frXv8573vOekvunyqnoM2cRkY2ImcAbwF9SSk80abIvsLDB60WFYy0dlyTpbeHSSy9l/vz5jBkzhosvvpgHHniAyZMn8+EPf5iRI0cCcPLJJ3PwwQczfPhwrrnmmvprBw4cyLJly1iwYAHDhg3j3HPPZfjw4Rx99NFs2LABaFyBGjhwIF/96lcZN24cI0eO5JlnngFg6dKlvPe972XcuHF88pOfZL/99mPZsmWN+llbW8vUqVMZMWIEI0eO5Ac/+AEA1157Le985zsZPXo0p512GuvXr69/3wsuuIDJkyczePBgHnzwQc455xyGDRvG1KlT6+/btWtXvvCFLzBu3DiOOuooli5dusV3NH36dCZNmsTBBx/MMcccw5IlS7b6nc6cOZNDDz2UUaNGccopp7BixQoAfvjDH3LQQQcxatQozjjjDAAefPBBxowZw5gxYxg7dixr1qzZvj9cGVQ0nKWUalNKY4B+wISIGNGkSXODsGkrx7cQEedFxLSImNbcH06SpJ3Rt7/9bfbff39mzpzJd7/7XQD+8Y9/8M1vfpO5c+cCcN111zF9+nSmTZvGD3/4Q5YvX77FfZ5//nk+/elPM2fOHHr06MGtt97a7Pv17t2bGTNmcMEFF9QPA37ta1/j3e9+NzNmzOCUU07hlVde2eK6mTNn8uqrr/L000/z1FNPcfbZZwNw6qmn8uSTTzJr1iyGDRvGz372s/prVqxYwd/+9jd+8IMfcOKJJ/L5z3+eOXPm8NRTTzFz5kwA1q1bx7hx45gxYwaTJk3ia19r/Oh6dXU1n/3sZ7nllluYPn0655xzDv/5n/+51e/0ox/9KN/5zneYPXs2I0eOrL/nt7/9bf75z38ye/Zsrr76agCuuOIKrrrqKmbOnMnDDz9Mp06dtnrvctohS2mklFZGxAPAscDTDU4tAvo3eN0PWAx0aOF4c/e+BrgGYPz48c0GOEmSWmvgpXeW/Z4Lvn18Ue0nTJjQaDmGH/7wh9x+++0ALFy4kOeff55evXo1umbQoEGMGTMGgIMPPpgFCxY0e+9TTz21vs1tt90GwCOPPFJ//2OPPZaePXtucd3gwYN58cUX+exnP8vxxx/P0UcfDcDTTz/Nf/3Xf7Fy5UrWrl3LMcccU3/NiSeeSEQwcuRI9tprr/pK4PDhw1mwYAFjxowhk8nwwQ9+EICzzjqrvn+bPfvsszz99NO8973vBfIVvL59+7b43a1atYqVK1cyadIkAD72sY/xgQ98AIBRo0Zx5plncvLJJ3PyyScDMHHiRP793/+dM888k1NPPZV+/fq1eO9yq1g4i4g+QHUhmHUC3gN8p0mzO4DPRMRNwCHAqpTSkohYCgyJiEHAq8AZwIcr1VdJkral2CBVCV26dKn//YEHHuC+++7jscceo3Pnzhx55JHNLtew22671f+ezWbrhzVbapfNZqmpqQHya3RtS8+ePZk1axb33HMPV111FTfffDPXXXcdU6dO5fe//z2jR4/m+uuv54EHHtjivTKZTKP+ZTKZ+vduqumMx5QSw4cP57HHHttmH7flzjvv5KGHHuKOO+7gG9/4BnPmzOHSSy/l+OOP56677uLQQw/lvvvu48ADD2z1e22PSg5r9gXuj4jZwJPknzn7U0ScHxHnF9rcBbwIvABcC3wKIKVUA3wGuAeYB9ycUppTwb5KktSudOvWbavPOa1atYqePXvSuXNnnnnmGR5//PGy9+Hwww/n5ptvBuDee++tf0aroWXLllFXV8dpp53GN77xDWbMmAHAmjVr6Nu3L9XV1dx4441Fv3ddXV39M3G//vWvOfzwwxudHzp0KEuXLq0PZ9XV1cyZ03JU6N69Oz179uThhx8G4IYbbmDSpEnU1dWxcOFCJk+ezH//93/XV/rmz5/PyJEjueSSSxg/fnz9c3g7QsUqZyml2cDYZo5f3eD3BHy6hevvIh/eJEl62+nVqxcTJ05kxIgRTJkyheOPb1y5O/bYY7n66qsZNWoUQ4cO5dBDm65W1Xpf/epX+dCHPsRvf/tbJk2aRN++fenWrVujNq+++ipnn302dXV1AHzrW98C4Bvf+AaHHHII++23HyNHjiz6gfouXbowZ84cDj74YLp3785vf/vbRuc7dOjALbfcwoUXXsiqVauoqanhoosuYvjw4S3e8xe/+AXnn38+69evZ/Dgwfz85z+ntraWs846i1WrVpFS4vOf/zw9evTgK1/5Cvfffz/ZbJaDDjqIKVOmFNX/1ojtKVnuLMaPH5+mTZvW1t2QJO0C5s2bx7Bhw9q6G23qrbfeIpvNUlVVxWOPPcYFF1xQ/8B+pXXt2pW1a9duu+FOorl/TxExPaU0vmlb99aUJEnNeuWVVzj99NOpq6ujQ4cOXHvttW3dpbcFw1kR/u/B+ey5+26cMnbHzdiQJKmtDBkyhH/+859t8t67UtWsWIazIixb+1Zbd0GSJO3iKroI7a4ml81QU7frPKMnSZLaH8NZEaqyGTbV1LV1NyRJ0i7McFaEDtmgps5wJkmSKsdwVoSqbIbqWoc1JUntU9euXQFYvHgx73//+5ttc+SRR7KtZaeuvPLK+o3KAY477jhWrlxZvo7uAA888AAnnHDCdrdfsGABI0Y03QK8bRjOipDLZqiutXImSWrf9tlnn/rV9UvRNJzddddd9OjRoxxd03YwnBUhlw3DmSRph7jkkkv48Y9/XP/68ssv53vf+x5r167lqKOOYty4cYwcOZI//OEPW1zbsAq0YcMGzjjjDEaNGsUHP/jBRntrXnDBBYwfP57hw4fz1a9+Fchvpr548WImT57M5MmTARg4cCDLli0D4Pvf/z4jRoxgxIgRXHnllfXvN2zYMM4991yGDx/O0Ucf3ewenr/73e8YMWIEo0eP5ogjjqi/9l3vehfjxo1j3LhxPProo0C+8jVp0iROP/10DjjgAC699FJuvPFGJkyYwMiRI5k/fz4AU6dO5fzzz+dd73oXBxxwAH/605+2eN9169Zxzjnn8M53vpOxY8c2+501tHHjRs4++2xGjhzJ2LFjuf/++wGYM2cOEyZMYMyYMYwaNYrnn3+edevWcfzxxzN69GhGjBixxU4GJUkp7TI/Bx98cKqkGx9/OV1yy6yKvockqX2YO3dum77/jBkz0hFHHFH/etiwYenll19O1dXVadWqVSmllJYuXZr233//VFdXl1JKqUuXLimllF566aU0fPjwlFJK3/ve99LZZ5+dUkpp1qxZKZvNpieffDKllNLy5ctTSinV1NSkSZMmpVmz8v+N22+//dLSpUvr33vz62nTpqURI0aktWvXpjVr1qSDDjoozZgxI7300kspm82mf/7znymllD7wgQ+kG264YYvPNGLEiLRo0aKUUkorVqxIKaW0bt26tGHDhpRSSs8991za/N/y+++/P3Xv3j0tXrw4bdy4Me2zzz7psssuSymldOWVV6bPfe5zKaWUPvaxj6Vjjjkm1dbWpueeey7tu+++acOGDen+++9Pxx9/fEoppS996Uv1/VmxYkUaMmRIWrt2baO+NfzOrrjiijR16tSUUkrz5s1L/fv3Txs2bEif+cxn0q9+9auUUkpvvfVWWr9+fbrlllvSJz7xifr7rFy5stm/Z3P/noBpqZk84zpnRchlg01WziTp7eny7hW456oWT40dO5Y33niDxYsXs3TpUnr27MmAAQOorq7my1/+Mg899BCZTIZXX32V119/nb333rvZ+zz00ENceOGFAIwaNYpRo0bVn7v55pu55pprqKmpYcmSJcydO7fR+aYeeeQRTjnlFLp06QLAqaeeysMPP8xJJ53EoEGDGDNmDAAHH3wwCxYs2OL6iRMnMnXqVE4//XROPfVUIL9h+Wc+8xlmzpxJNpvlueeeq2//zne+k759+wKw//77c/TRRwMwcuTI+moWwOmnn04mk2HIkCEMHjx4i03K7733Xu644w6uuOIKIF8Ze+WVV1rcnuuRRx7hs5/9LAAHHngg++23H8899xyHHXYY3/zmN1m0aBGnnnoqQ4YMYeTIkXzxi1/kkksu4YQTTuBd73pXi9/f9jKcFSGXzVDjhABJenvaSpCqlPe///3ccsstvPbaa5xxxhkA3HjjjSxdupTp06eTy+UYOHAgGzdu3Op9ImKLYy+99BJXXHEFTz75JD179mTq1KnbvE/ayn7cu+22W/3v2Wy22WHNq6++mieeeII777yTMWPGMHPmTH70ox+x1157MWvWLOrq6ujYsWOz98xkMvWvM5kMNTU1LX6+pq9TStx6660MHTp0q59vW5/zwx/+MIcccgh33nknxxxzDD/96U9597vfzfTp07nrrrv40pe+xNFHH81ll122Xe/TEp85K0J+EVorZ5KkHeOMM87gpptu4pZbbqmffblq1Sr23HNPcrkc999/Py+//PJW73HEEUdw4403AvD0008ze/ZsAFavXk2XLl3o3r07r7/+OnfffXf9Nd26dWPNmjXN3uv3v/8969evZ926ddx+++1FVYrmz5/PIYccwte//nV69+7NwoULWbVqFX379iWTyXDDDTdQW1u73ffb7He/+x11dXXMnz+fF198cYsQdswxx/CjH/2oPnRta0uqht/Zc889xyuvvMLQoUN58cUXGTx4MBdeeCEnnXQSs2fPZvHixXTu3JmzzjqLL37xi8yYMaPo/jdl5awIVdlgU42VM0nSjjF8+HDWrFnDvvvuWz+8d+aZZ3LiiScyfvx4xowZw4EHHrjVe1xwwQWcffbZjBo1ijFjxjBhwgQARo8ezdixYxk+fDiDBw9m4sSJ9decd955TJkyhb59+zYaPhw3bhxTp06tv8cnPvEJxo4d2+wQZnMuvvhinn/+eVJKHHXUUYwePZpPfepTnHbaafzud79j8uTJ9UOmxRg6dCiTJk3i9ddf5+qrr25UfQP4yle+wkUXXcSoUaNIKTFw4MBmJw5s9qlPfYrzzz+fkSNHUlVVxfXXX89uu+3Gb3/7W371q1+Ry+XYe++9ueyyy3jyySe5+OKLyWQy5HI5fvKTnxTd/6ZiayXKnc348ePTttZuaY37n3mDXzy2gOvPnlCx95AktQ/z5s1r8ZkktR9Tp07lhBNOaHFdt/aiuX9PETE9pTS+aVuHNYtQ5VIakiSpwhzWLELOHQIkSWpXrr/++rbuQtlZOSuCi9BKkqRKM5wVoSrjUhqS9HayKz2XrbZT7L8jw1kR3FtTkt4+OnbsyPLlyw1oapWUEsuXL99iBunW+MxZERzWlKS3j379+rFo0SKWLl3a1l3RTq5jx47069dvu9sbzoqQX4TW/wclSW8HuVyOQYMGtXU39DbksGYRqrJBdY2VM0mSVDmGsyJ0yGaotnImSZIqyHBWhConBEiSpAoznBUhlw2X0pAkSRVlOCtCLpthk5UzSZJUQYazIlRlghrDmSRJqiDDWRGymSABtU4KkCRJFWI4K0JEkMs4KUCSJFWO4axI7hIgSZIqyXBWpKqsm59LkqTKMZwVKZfNUF1n5UySJFWG4axI+WFNK2eSJKkyDGdFymUzLqchSZIqxnBWpConBEiSpAoynBUpv5SGw5qSJKkyDGdFylVZOZMkSZVjOCtSlZUzSZJUQYazInXIukOAJEmqHMNZkaqy4SK0kiSpYqoqdeOI6A/8EtgbqAOuSSn9T5M2FwNnNujLMKBPSunNiFgArAFqgZqU0vhK9bUYLkIrSZIqqWLhDKgBvpBSmhER3YDpEfGXlNLczQ1SSt8FvgsQEScCn08pvdngHpNTSssq2Mei5bJBdY3hTJIkVUbFhjVTSktSSjMKv68B5gH7buWSDwG/qVR/yiWXzVBT57CmJEmqjB3yzFlEDATGAk+0cL4zcCxwa4PDCbg3IqZHxHmV7uP2qnJCgCRJqqBKDmsCEBFdyYeui1JKq1todiLw9yZDmhNTSosjYk/gLxHxTErpoWbufx5wHsCAAQPK3Pst5TLurSlJkiqnopWziMiRD2Y3ppRu20rTM2gypJlSWlz43zeA24EJzV2YUrompTQ+pTS+T58+5en4VuSsnEmSpAqqWDiLiAB+BsxLKX1/K+26A5OAPzQ41qUwiYCI6AIcDTxdqb4WI7+UhuFMkiRVRiWHNScCHwGeioiZhWNfBgYApJSuLhw7Bbg3pbSuwbV7Abfn8x1VwK9TSn+uYF+3Wy6bYZPDmpIkqUIqFs5SSo8AsR3trgeub3LsRWB0RTrWSjkrZ5IkqYLcIaBILqUhSZIqyXBWpKpshk0uQitJkirEcFakDtmgxu2bJElShRjOipRfhNZhTUmSVBmGsyK5zpkkSaokw1mRctkwnEmSpIoxnBWpKpOhxmFNSZJUIYazIuWywSYrZ5IkqUIMZ0XKZa2cSZKkyjGcFSm/CK2VM0mSVBmGsyJVZYNNNVbOJElSZRjOitTBypkkSaogw1mRqlxKQ5IkVZDhrEg5dwiQJEkVZDgrkovQSpKkSjKcFclFaCVJUiUZzork3pqSJKmSDGdFclhTkiRVkuGsSPlFaB3WlCRJlWE4K1JVNqiusXImSZIqw3BWpA7ZDNVWziRJUoUYzopU5YQASZJUQYazIuWy4VIakiSpYgxnRcplM2yyciZJkirEcFakqkxQYziTJEkVYjgrUjYTJKDWSQGSJKkCDGdFighyGScFSJKkyjCclcBdAiRJUqUYzkpQlXXzc0mSVBmGsxLkshmq66ycSZKk8jOclSA/rGnlTJIklZ/hrAS5bMblNCRJUkUYzkpQ5YQASZJUIYazEuSX0nBYU5IklZ/hrAS5KitnkiSpMgxnJaiyciZJkirEcFaCDll3CJAkSZVhOCtBVTZchFaSJFWE4awELkIrSZIqxXBWglw2qK4xnEmSpPIznJUgl81QU+ewpiRJKj/DWQmqnBAgSZIqpGLhLCL6R8T9ETEvIuZExOeaaXNkRKyKiJmFn8sanDs2Ip6NiBci4tJK9bMUuYx7a0qSpMqoquC9a4AvpJRmREQ3YHpE/CWlNLdJu4dTSic0PBARWeAq4L3AIuDJiLijmWvbRM7KmSRJqpCKVc5SSktSSjMKv68B5gH7buflE4AXUkovppQ2ATcB76tMT4uXX0rDcCZJkspvhzxzFhEDgbHAE82cPiwiZkXE3RExvHBsX2BhgzaL2P5gV3G5bIZNDmtKkqQKqOSwJgAR0RW4FbgopbS6yekZwH4ppbURcRzwe2AIEM3cqtk0FBHnAecBDBgwoGz93pqclTNJklQhFa2cRUSOfDC7MaV0W9PzKaXVKaW1hd/vAnIR0Zt8pax/g6b9gMXNvUdK6ZqU0viU0vg+ffqU/TM0x6U0JElSpVRytmYAPwPmpZS+30KbvQvtiIgJhf4sB54EhkTEoIjoAJwB3FGpvharKpthk4vQSpKkCqjksOZE4CPAUxExs3Dsy8AAgJTS1cD7gQsiogbYAJyRUkpATUR8BrgHyALXpZTmVLCvRemQDTZU17Z1NyRJ0i6oYuEspfQIzT871rDN/wL/28K5u4C7KtC1VqvKZqjeWNPW3ZAkSbsgdwgoQVUmXOdMkiRVhOGsBB2qXIRWkiRVhuGsBFWZDDWucyZJkirAcFaCXDbYZOVMkiRVgOGsBLmslTNJklQZhrMS5BehtXImSZLKz3BWgqpssKnGypkkSSo/w1kJOlg5kyRJFWI4K0FV1nXOJElSZRjOSpDLZqh2QoAkSaoAw1kJclbOJElShRjOSuAitJIkqVIMZyXID2taOZMkSeVnOCuBw5qSJKlSDGclyC9C67CmJEkqP8NZCaqyQXWNlTNJklR+hrMSdMhmqLZyJkmSKsBwVoIqJwRIkqQKMZyVIJcNl9KQJEkVYTgrQS6bYZOVM0mSVAGGsxJUZYIaw5kkSaoAw1kJspkgAbVOCpAkSWVmOCtBRJDLOClAkiSVn+GsRO4SIEmSKsFwVqKqrJufS5Kk8jOclSiXzVBdZ+VMkiSVl+GsRPlhTStnkiSpvAxnJcplMy6nIY2QAeUAACAASURBVEmSys5wVqIqJwRIkqQKMJyVKL+UhsOakiSpvAxnJcpVWTmTJEnlZzgrUZWVM0mSVAGGsxJ1yLpDgCRJKj/DWYmqsuEitJIkqewMZyVyEVpJklQJhrMS5bJBdY3hTJIklZfhrES5bIaaOoc1JUlSeRnOSlTlhABJklQBhrMS5TLurSlJksrPcFainJUzSZJUAYazEuWX0jCcSZKk8jKclSiXzbDJYU1JklRmFQtnEdE/Iu6PiHkRMSciPtdMmzMjYnbh59GIGN3g3IKIeCoiZkbEtEr1s1Q5K2eSJKkCqip47xrgCymlGRHRDZgeEX9JKc1t0OYlYFJKaUVETAGuAQ5pcH5ySmlZBftYMpfSkCRJlVCxcJZSWgIsKfy+JiLmAfsCcxu0ebTBJY8D/SrVn3KrymbY5CK0kiSpzHbIM2cRMRAYCzyxlWYfB+5u8DoB90bE9Ig4r3K9K02HbFDj9k2SJKnMKjmsCUBEdAVuBS5KKa1uoc1k8uHs8AaHJ6aUFkfEnsBfIuKZlNJDzVx7HnAewIABA8re/5ZUZTNUb6zZYe8nSZLeHipaOYuIHPlgdmNK6bYW2owCfgq8L6W0fPPxlNLiwv++AdwOTGju+pTSNSml8Sml8X369Cn3R2hRVSZc50ySJJVdJWdrBvAzYF5K6fsttBkA3AZ8JKX0XIPjXQqTCIiILsDRwNOV6mspOlS5CK0kSSq/Sg5rTgQ+AjwVETMLx74MDABIKV0NXAb0An6cz3LUpJTGA3sBtxeOVQG/Tin9uYJ9LVpVJkON65xJkqQyq+RszUeA2EabTwCfaOb4i8DoLa9oP3LZYJOVM0mSVGbuEFCiXNbKmSRJKj/DWYnyi9BaOZMkSeVlOCtRVTbYVGPlTJIklZfhrEQdrJxJkqQKMJyVqCrrOmeSJKn8DGclqspkqHZCgCRJKjPDWYk6VFk5kyRJ5Wc4K5GL0EqSpEownJUol3X7JkmSVH6GsxLlnBAgSZIqwHBWovwitA5rSpKk8jKclagqG1TXWDmTJEnlZTgrUYdshmorZ5IkqcwMZyWqckKAJEmqAMNZiaqy4VIakiSp7AxnJeqQzbDJypkkSSozw1mJqjJBjeFMkiSVmeGsRNlMkIBaJwVIkqQyMpyVKCLIZZwUIEmSystw1gruEiBJksrNcNYKVVk3P5ckSeVlOGuFXDZDdZ2VM0mSVD6Gs1bID2taOZMkSeVjOGuFXDbjchqSJKmsDGetUOWEAEmSVGaGs1bIL6XhsKYkSSofw1kr5KqsnEmSpPIynLVClZUzSZJUZoazVuiQdYcASZJUXoazVqjKhovQSpKksjKctYKL0EqSpHIznLVCLhtU1xjOJElS+RjOWiGXzVBT57CmJEkqH8NZK1Q5IUCSJJWZ4awVchn31pQkSeVlOGuFnJUzSZJUZoazVsgvpWE4kyRJ5WM4a4VcNsMmhzUlSVIZGc5aIWflTJIkldl2hbOI6BIRmcLvB0TESRGRq2zX2j+X0pAkSeW2vZWzh4COEbEv8FfgbOD6SnVqZ1GVzbDJRWglSVIZbW84i5TSeuBU4EcppVOAgyrXrZ1Dh2xQ4/ZNkiSpjLY7nEXEYcCZwJ2FY1WV6dLOI78IrcOakiSpfLY3nF0EfAm4PaU0JyIGA/dv7YKI6B8R90fEvIiYExGfa6ZNRMQPI+KFiJgdEeManDs2Ip4tnLu0mA+1o1RlwnXOJElSWW1X9Sul9CDwIEBhYsCylNKF27isBvhCSmlGRHQDpkfEX1JKcxu0mQIMKfwcAvwEOCQissBVwHuBRcCTEXFHk2vbXIcqF6GVJEnltb2zNX8dEbtHRBdgLvBsRFy8tWtSSktSSjMKv68B5gH7Nmn2PuCXKe9xoEdE9AUmAC+klF5MKW0Cbiq0bVeqMhlqHNaUJElltL3DmgellFYDJwN3AQOAj2zvm0TEQGAs8ESTU/sCCxu8XlQ41tLx5u59XkRMi4hpS5cu3d4ulUUuG2yyciZJkspoe8NZrrCu2cnAH1JK1cB2lYwioitwK3BRIeA1Ot3MJWkrx7c8mNI1KaXxKaXxffr02Z4ulU0ua+VMkiSV1/aGs/8DFgBdgIciYj+gadDaQiHQ3QrcmFK6rZkmi4D+DV73AxZv5Xi7kl+E1sqZJEkqn+0KZymlH6aU9k0pHVd4PuxlYPLWromIAH4GzEspfb+FZncAHy3M2jwUWJVSWgI8CQyJiEER0QE4o9C2XanKBptqrJxJkqTy2a7ZmhHRHfgqcETh0IPA14FVW7lsIvnn0p6KiJmFY18m/7waKaWryT+/dhzwArCe/M4DpJRqIuIzwD1AFrgupTRn+z/WjtHBypkkSSqz7V1I9jrgaeD0wuuPAD8nv2NAs1JKj9D8s2MN2yTg0y2cu4t8eGu3qrKucyZJkspre8PZ/iml0xq8/lqDatjbVlXGHQIkSVJ5be+EgA0RcfjmFxExEdhQmS7tPDpUWTmTJEnltb2Vs/OBXxaePQNYAXysMl3aebgIrSRJKrft3b5pFjA6InYvvF4dERcBsyvZufYul3X7JkmSVF7bO6wJ5ENZg4Vk/70C/dmp5JwQIEmSyqyocNbEVmdivh3kF6F1WFOSJJVPa8LZ2z6VVGWD6horZ5IkqXy2+sxZRKyh+RAWQKeK9Kg9q6uDVAvZHJBfhLbaypkkSSqjrYazlFK3HdWRncI9X4Ye/eGw/Lq5VU4IkCRJZdaaYc23n869YN2y+pdV2XApDUmSVFaGs2J06QXr/xXOOmQzbLJyJkmSyshwVozOvWH9m/UvqzJBjeFMkiSVkeGsGF16NxrWzGaCBNQ6KUCSJJWJ4awYnXs3GtaMCHIZJwVIkqTyMZwVo8mEAHCXAEmSVF6Gs2J06glvrYHa6vpDVVk3P5ckSeVjOCtGJpMPaA0mBeSyGarrrJxJkqTyMJwVq0vj587yw5pWziRJUnkYzorVuXczC9FaOZMkSeVhOCtWk4Voc27hJEmSyshwVqzOvWHd8vqX+aU0HNaUJEnlYTgrVpfesL5BOKtyKQ1JklQ+hrNiNVmItsrKmSRJKiPDWbE679FoQkAHnzmTJEllZDgrVpNhzfxsTStnkiSpPAxnxWqylIaL0EqSpHIynBWruUVoawxnkiSpPAxnxercCzasgEK1LJfNUFPnsKYkSSoPw1mxsjno0AU2rgTyG587IUCSJJWL4awUDZ47y2XcW1OSJJWP4awUDZ47c/smSZJUToazUnT+13IabnwuSZLKyXBWigYL0eayGTY5rClJksrEcFaKRsOaVs4kSVL5GM5K0bk3rMsPa7qUhiRJKifDWSkaVM6qshk2uQitJEkqE8NZKRospdEhG9S4fZMkSSoTw1kpuvRqVDlznTNJklQuhrNSNHjmrGNVhg2batu4Q5IkaVdhOCvF5mfOUmKfHp14deWGtu6RJEnaRRjOSpHrBJkcvLWG/nt0ZuGb69u6R5IkaRdhOCtV516wfnk+nK1YT0o+dyZJklqvYuEsIq6LiDci4ukWzl8cETMLP09HRG1E7FE4tyAiniqcm1apPrZKl3w467pbFZ07VLF07Vtt3SNJkrQLqGTl7Hrg2JZOppS+m1Iak1IaA3wJeDCl9GaDJpML58dXsI+la7CcRv+enVj4ps+dSZKk1qtYOEspPQS8uc2GeR8CflOpvlREg4Vo++3RmUUrfO5MkiS1Xps/cxYRnclX2G5tcDgB90bE9Ig4bxvXnxcR0yJi2tKlSyvZ1cY696qvnA3YozOvLDecSZKk1mvzcAacCPy9yZDmxJTSOGAK8OmIOKKli1NK16SUxqeUxvfp06fSff2XBpWz/j3zkwIkSZJaqz2EszNoMqSZUlpc+N83gNuBCW3Qr61rsBBt/z185kySJJVHm4aziOgOTAL+0OBYl4jotvl34Gig2RmfbapB5WzAHp15xbXOJElSGVRV6sYR8RvgSKB3RCwCvgrkAFJKVxeanQLcm1Ja1+DSvYDbI2Jz/36dUvpzpfpZsgazNffp0Ymla96iuraOXLY9FCMlSdLOqmLhLKX0oe1ocz35JTcaHnsRGF2ZXpVR5z3qK2e5bIY+3XZjycqNDOjVuY07JkmSdmaWeUrVpTes/9cchv57dHJSgCRJajXDWal22x1qN0H1RiA/Y9PnziRJUmsZzkoVUdhfs7CchhugS5KkMjCctUbDLZz26MTCFS6nIUmSWsdw1hpdermchiRJKivDWWs0XIi2Z2cWGc4kSVIrGc5ao8FCtH267ca6TTWse6umjTslSZJ2Zoaz1mjwzFlE0M89NiVJUisZzlqjwUK0AP17usemJElqHcNZa3T51zNn4HIakiSp9QxnrdG5N6z/VzgbsIfDmpIkqXUMZ63RYEIAkH/mzMqZJElqBcNZazSYEACFhWh95kySJLWC4aw1OvWEt9ZAbTVQeOZsxXpSSm3cMUmStLMynLVGJpMPaOvfBGD3jjk6VGVYvm5TG3dMkiTtrAxnrdXkubP+PncmSZJawXDWWk2eO8vP2PS5M0mSVBrDWWs1WYi23x6drJxJkqSSGc5aq+lCtA5rSpKkVjCctVaThWj7uxCtJElqBcNZa3XfF1YsqH85YI/OvGLlTJIklchw1lr9D4GFj9e/3KdHR15f9RY1tXVt2ClJkrSzMpy1Vu+hsGElrHkdgN2qsvTq2oElqza2ccckSdLOyHDWWpnMFtUznzuTJEmlMpyVw4BD4JUG4cwZm5IkqUSGs3Lof2jjcOYG6JIkqUSGs3LYdxwsfQY2rQPyMzYXLF/Xxp2SJEk7I8NZOeQ6wV7D4dXpALxz4B48On+5MzYlSVLRDGflMuBQeOUJID8hoH/PTjz24vJtXCRJktSY4axc+h8KrzxW//L4UX25c/aSNuyQJEnaGRnOymXAobBoGtTVAnDcyL7cO/d1qh3alCRJRTCclUuX3tC1D7wxF4B+PTszYI/OPDbfoU1JkrT9DGflNKDxkhonOLQpSZKKZDgrp/6HwsIn6l9OGdmXe+a+5tCmJEnaboazcmpSOdu3RycG9e7C319Y1oadkiRJOxPDWTn1egdUr4dVi+oPHT+yL3c95dCmJEnaPoazcorYYiunzbM2N9U4tClJkrbNcFZuAxo/d7ZPj07s36crf5/v0KYkSdo2w1m5NXnuDPJDm87alCRJ28NwVm59R8Py+fDWmvpDU0buzX3zHNqUJEnbZjgrt6rd8gFt0ZP1h/p278Q7+nR11qYkSdomw1klNDe0Oaovf5y9uI06JEmSdhYVC2cRcV1EvBERT7dw/siIWBURMws/lzU4d2xEPBsRL0TEpZXqY8UMngTP3AUp1R86YdQ+/O2ZN3h15YY27JgkSWrvKlk5ux44dhttHk4pjSn8fB0gIrLAVcAU4CDgQxFxUAX7WX6DJkGqhRf+Wn+oT7fdOOuQ/fj+vc+1YcckSVJ7V7FwllJ6CHizhEsnAC+klF5MKW0CbgLeV9bOVVoEHP55eOQHjQ5/ctJgHnxuKfOWrG6jjkmSpPaurZ85OywiZkXE3RExvHBsX2BhgzaLCsd2LsNPhVWvwMJ/1B/q1jHHZybvz7fvfqYNOyZJktqztgxnM4D9UkqjgR8Bvy8cj2bapmaO5RtHnBcR0yJi2tKlSyvQzRJlq+DfLoRHrmx0+MOH7MeC5et41JmbkiSpGW0WzlJKq1NKawu/3wXkIqI3+UpZ/wZN+wEtTnNMKV2TUhqfUhrfp0+fiva5aGPPyi+p8ca/KmUdqjJ88eihfOvuZ6irazFzSpKkt6k2C2cRsXdEROH3CYW+LAeeBIZExKCI6ACcAdzRVv1slVwnOOST8Pf/aXT4+JF9yQT8yQ3RJUlSE5VcSuM3wGPA0IhYFBEfj4jzI+L8QpP3A09HxCzgh8AZKa8G+AxwDzAPuDmlNKdS/ay4d34CnrsbVv7rMbpMJrh0yjCuuOdZ3qqpbcPOSZKk9iZS2nWG1saPH5+mTZvW1t3Y0r1fgdpqmPLtRofP/vk/eNeQPpxz+KA26pgkSWorETE9pTS+6fG2nq359nDop2DWb2Dd8kaHL50yjB8/8AJLVrkwrSRJyjOc7Qi794WD3gf/+L9Gh4fu3Y2zJw7iwt/8k+paN0WXJEmGsx1n4ufgH9fCmy82OnzBpP3p3KGKK+59to06JkmS2hPD2Y7Sa3844mK47Tyorak/nMkEP/jgGP44czF/nfd6G3ZQkiS1B4azHemQ86FDV3j4ikaH9+jSgR99eCyX3DqbRSvWt1HnJElSe2A425EyGTj5J/Dkz2Dhk41OHbzfHpx3xGA+/et/sqnG588kSXq7MpztaLv3heO/B7edC2+taXTq3HcNpk/XDu69KUnS25jhrC0cdBIMnAh/vrTR4Yjgex8Yw1+feZ0bHn+5jTonSZLakuGsrRz7HVjwd5jbeGeq7p1z3HDOIVz1txf4w8xX26hzkiSprRjO2spuXeG0n8Kd/w5Ln2t0akCvzvzy4xP4xp/mOYNTkqS3GcNZW+o3Ht77DfjVabC68SboB+zVjZ9+bDz/cctsHn9xeQs3kCRJuxrDWVsb8yEYf3Y+oG1Y2fhU/x786MNj+fSNM5i9aGULN5AkSbsSw1l7cPjnYeDhcNOZUL2x0al/27833z5tFOdcP425i1e3UQclSdKOYjhrDyLg2G9Bl95w+3lQV9vo9HsP2ouvv284H73uCWa8sqKNOilJknYEw1l7kcnCKf8H65bnl9hIqdHp40b25bsfGM25v5jGoy8sa6NOSpKkSjOctSe5jnDGjfDK43DXxVDXeKeAyUP35Kozx/HZ3/yT++Y6i1OSpF2R4ay96dQDpv4J3pib30WgZlOj04cO7sV1U9/Jpbc95TpokiTtggxn7VHH7nDWrVC9Hm76MGxqvBn66P49uPETh/D/7prHTx6YT2oyBCpJknZehrP2KtcJTr8BuvSBG06GDY0nAgzduxu3f2oi98x5jXN/OZ1VG6rbqKOSJKmcDGftWbYK3ncV7Dsefn4crFzY6PQ+PTpx8ycPo1/PTpz4o0d4+tVVbdRRSZJULoaz9i6TgWO+CWPOhJ8eBS8+0Oh0h6oMl580nP84digfu+4f3PSPVxzmlCRpJ2Y42xlEwL99Jr8X523nwcPf32KpjRNG7cNvP3kY1/39JT77m3+yYt2mFm4mSZLaM8PZzmTQEXDu3+CZP8Fvz4KNjXcMeMeeXfnDpw9nr907csyVD3HPnNfaqKOSJKlUhrOdTfd+cPbd+YkC106G155udLpThyxfOeEgrjpzHN+6ax6fu8kqmiRJOxPD2c6oajc48Uo44mL45Unw8PegtqZRk3cO3IO7P3cEe3TpwLH/8xB/nLWYujqfRZMkqb2LXenh8fHjx6dp06a1dTd2rJUL4Q+fguoN+e2feu2/RZN/vPQm3/jTXGrrEl885gAmD92TiGiDzkqSpM0iYnpKaXzT41bOdnY9+sNH/gAjPwA/fQ88cc0W2z5NGLQHd3xmIhceNYRv3/0M77/6MR6d7/6ckiS1R1bOdiXLXoDfnw+RhSnfgX3GbNGkti7xx1mL+cF9z9G3e0c+ecT+TDqgD5mMlTRJknaklipnhrNdTV0t/PNX8Lf/Dw48Dt79FejSe4tm1bV1/Gn2Yq596CU21dbxicMHcfLYfemYy7ZBpyVJevsxnL3dbFgJD3wbnroZJl0C4z+e33GgiZQSj85fzrUPv8jTr67mY4ftx8cmDmT3jrk26LQkSW8fhrO3qzfmwZ8vhVWvwlFfgWEn5Re1bcZzr6/h6gfn88CzS/noYftx9sRBdO9kSJMkqRIMZ29nKcH8v8J9l0MmB++5HAZParH5gmXr+N/7X+Cv817nI4cN5OMTB9G9syFNkqRyMpwpP4tzzm3559F6DsxX0vY9uMXmLy9fx//+7QXum/c65x2xP2dPHOgzaZIklYnhTP9SWw0zfgEP/wB6vwMO/zwMmtTicOeLS9fy339+ltmLVvKFo4dy8th9yTq7U5KkVjGcaUs1m+Cp38Hfr4QOXfMh7cATINP88nfTFrzJ/7trHhuq6/iPY4dyxJA+hjRJkkpkOFPL6urym6k/8n14aw1MOA9Gfwg67r5F05QS98x5jSvve57XVm/ksMG9OHxIb971jj4M6NW5DTovSdLOyXCmbUsJXn4UnrwW5t+f33VgwrnQZ2izzV9btZG/v7CMRwo/nXJZPnBwPz58yAB6dd1tB3dekqSdi+FMxVm9GKb9HKZfD3seCAefnR/yrOrQbPOUEnMWr+aGx17m7qeXMGVEX845fBBD9+62Y/stSdJOwnCm0tS8BfP+mA9pS5/JD3eO+1h+IkELlq99ixufeIUbHn+ZoXt14+Sx+zLpgD706WY1TZKkzQxnar1lL+Rnec76DfQ5MB/Uhp3Y7LNpAG/V1HL3U69x79zXeOT5ZezXqwtHDu3DkUP3ZEz/Hk4mkCS9rRnOVD41m+DZu2D2zbDgYXjHe2D0GbD/uyHb/GK11bV1zHh5Bfc/u5T7n3mDFes3cfyovpw4eh/G9u9BtLCMhyRJuyrDmSpj/Zv5hW1n3wzL58OwE+Cgk2Hgu5rdy3OzF95Yyx9nLeaPsxZTXVfHiaP24cihe3LAXl3p0bn559okSdqV7PBwFhHXAScAb6SURjRz/kzgksLLtcAFKaVZhXMLgDVALVDTXMebYzhrYysWwJzfw9zfw8qFcODxMHxzUGu+orZ5IsEfZy3miZfe5IU31tKpQ5YD9urKkD27MapfdyYP3ZOeXQxskqRdS1uEsyPIh65fthDO/g2Yl1JaERFTgMtTSocUzi0AxqeUlhXznoazdmTFApj7h3xYW/ESDDkahh6XHwLdrWuLl6WUWLxqI8+/vobnX1/Lkwve5LH5yxm+7+6896C9Ofqgvei/h+upSZJ2fm0yrBkRA4E/NRfOmrTrCTydUtq38HoBhrNdx+rF+WfUnrkTFj4J+x0GQ6fAkGOg+77bvHxjdS2PPL+Me+e+xl/nvUHvrrsx8R29OXxILyYM6kXX3VoePpUkqb1q7+Hsi8CBKaVPFF6/BKwAEvB/KaVrtuf9DGc7gY2r4Pm/wHP3wAv3we775KtqBxwL/cZDZusbq9fWJWYtWsmjhYVvZy9axbC+uzPxHb2ZdEAfZ4FKknYa7TacRcRk4MfA4Sml5YVj+6SUFkfEnsBfgM+mlB5q4frzgPMABgwYcPDLL79c3g+hyqmrhUVP5oPa8/fC6ldh8JGw/1H5mZ/bUVXbsKmWaS+/ySPPL+PB55by+uqNHD6kD5MO6MMRB/Rmz24dK/4xJEkqRbsMZxExCrgdmJJSeq6FNpcDa1NKV2zr/ayc7eRWL4b5f4MX/govPgBd94L9J+cnFOx3GHTquc1bLFm1gYeeW8oDzy7lkReW0aVDFcP6duPAvrtz4N7dGNZ3d/bv09XqmiSpzbW7cBYRA4C/AR9NKT3a4HgXIJNSWlP4/S/A11NKf97W+xnOdiF1tbB4Jrz4N1jwCCyaBr32zwe1zWGtY/et36Iu8erKDcxbsppnXlvDM6+tZu7i1Sxft4lxA3ryzoE9GT9wD8b070HH3NaHUyVJKre2mK35G+BIoDfwOvBVIAeQUro6In4KnAZsHoesSSmNj4jB5KtpAFXAr1NK39ye9zSc7cJqNsGr0/NBbcFD8OoM6PUOGHh4/mfAYdCpx3bdatnat5j+8gqmLXiTJxes4NnX1jCodxcO7NuNYXvvzoF9u3Hg3ru73ZQkqaJchFa7lpq38gFtwSP5XQpenQ49B0L/Q2DAofmf7v1hO3Ye2Fhdy3Ovr2HektXMW5KvsM1bsoZsJhiyZ1cO2Ktbft21vbpxwF7d2MM11yRJZWA4066tthqWzIaFj8P/396dx8aZ3/cdf3/nvjgzvEmJ1LFarb1HrLWx2bppURhJgTpuUBcIiqzRoIHbIkCQIk7Ry27/CAq0fxQoitaom8JN3SRoYKNI08QIEtfBJqhTdLveTb3eXe2lYymREo/hMTOc+/r1j98zJFcrrUiJ1IzEzwt48Mw885B8uD+s9NHv+l5/Ca6/DKEIzP8ozD0Pcz8Ksxcgur8FAs451rabvLe6zXurFS6tbvPe6jaX1ipEQsa5yQyPT2U4N5nh/HSGp05ktfhAREQOROFMjhfn/Oa3S6/C4vf9qtD192DqSTjxSZh9Fk486wu436F6we2/raNQaXJ5rcKVQpUraxXeXdnmreUy0XCIp05keWo2y4+czPHcmVGmswpsIiJyewpnIq0aLL8GN3/gFxssvwalJR/YZp/1PWuzF/z7yMHmmznnWC41eOtmmYs3y7y+VOTPrm+RTUR57swoz58Z41OnRzk9niIe0eIDERFROBO5veY2rLwByz8Mjtdh8ypMPA4zn4DpZ2DmGX9OjR3oW/d6jiuFCq8sbPHKwiavLRa5sVVnIhPj1HiK02NpTo2nODeZ5txkhtPjaWKR0BH9oiIiMmwUzkT2q12H1bdg5XVYfRNW3oTVi5DIwsyPBMcn/Dl/GkL7D1Sdbo/lUoNrGzWub9a4tlHlSqHK1UKFpWKdk/kk5ybTnBpLMz+W5NRYivmxFPOjKZIx9biJiDxKFM5E7kevB8VrQVh7w/ewrbwBzbIfBp14AiY/5s8TT0D+1F1LUd2q1elxfbPK5bUqi5s+vC1u+fPSVp18MsqZcd/bdmY8xenx9M6iBPW4iYg8fBTORI5CbRPW3oLCu7B+CdaDc23Dh7XpYEh0+ml/HHBotK/Xc6yUfY/btY0q1zZrLKxXubRWYWmrxpnxNE/N+j3aHpvIMJNLMJNLMJaKEVI1BBGRoaRwJvIgNbdh7W3f07Z60Q+Nrr3lFxpMfAwmn9hzfgKyJ/e1J9vtNNpdLq1WeHu5zFvLZa5tVFkpN1kp1am2ukyNxDk1luLjM1menPUlrM5PZ7QwHgGd+wAAFqhJREFUQURkwBTORAbNOdhegcI7fluPwrv+2LgEzYpfhDB+fneIdPLjvmTVAbb6uFWj3WW13GBhoxZssuuPaxs1TuSTZBMRMokImXiETDzKaCrK0yezXJjLc2Y8rV43EZEjpHAmMswaJVi/7INa4V0f3tbehvINX/lg8uMwcd6XrBo750PbPQ6Rgg9tS1t1Ks0OlUaHSrPNdqPDeqXFmzdKvLZYZLvR5sJ8nmdO5pgfTTGTizOTTTKTSzCaimL32NMnIiKewpnIw6hd93PYCu/AxmXYuOLPm1f9goOdnrYn9ixGOA3hyH3/6MJ2k9eXirx5o8zNYp2VcoOVUoOVcoNmp8vHZrJcmMtxYS7Phfkcj01k1NMmInIACmcijxLnoFrwQW1nMcJ7fkHC9irk5/f0sj3mz2Nnfb3RA64ivZ1Ks8NbN8v8cLHID5eKvL5UYrPa4kQ+wVg6tueIM5mJMTmSYHIkztRInMmROImo5ruJiCiciRwX7YYvXbVxBTavBOersPm+D3S5OR/URs/ecj4D0eQ9/9hircVyqcFWtcVGtcVmcF6vNFkrNylUmhTKDQqVJrlkzG++G2wFcm4yzdxoimwiwkgiSiIa0rCpiDzy7hTO7n/sQ0SGSzTh916bevLDn7Ubfr+2fljbeh+u/Ik/Fxf9PLbxx/2ctrFzu69Hz9y1pFU+FSOfit318Xo9X1T+SqES1Cit8MfvrHKz2GA7mP/W7joy8Qhj6Rhzo0lO76mocHo8FSxmuPeFEiIiw0w9ZyLi9bq+1mi/t22n5+0ylG5AZtr3sI099uEet/jIoT5Kq9Oj2uywUW0GlRRqO1UVrm/WWC7WMTNmcwlm88mdygrnp0c4P5VhNpdQz5uIDD0Na4rIvet2oLQY9Lj1e90WfI/b1oIfDh096+e6ZU/6uW25k34INX/6vlaW3o5zjnKjw3KpznKxwdJWjSuFKpfWtrm0WqHW6vLYZJqRRIRoOEQsHCIW8cfkSJy50RRzo0nmR5OczKs0logMhsKZiBwN56Cy5oNaacmHuNINvw1IcdGHt3BkdwuQsWCYNHvCB7iRE34o9hCVam2urFeoNbu0uz2anR6tbo9mu0uh0mRxs87SVo0bW3WWinVyyehOSazTYylOT6SZySYYSUSCI0omHiGs1agicog050xEjoYZjEz743Z2VpbuWaBw6btQvgnlJb8xb3zE97aNnvFHf9g0fwpGZg8c3nKpKJ86Nbqve3s9x+p2g4X1Gtc3qyxs1PjDN5YpbDfZbnTYbvg94KqtDqOpGOcmMzw2mfYLGaZ8kfrRVJRsMko0rBqnInL/1HMmIoPV60FtHYrXd4dKNxf86+I1qKxCNOVD2siMP2dPBMfJoAduDpKj91wCa3+P6ShU/EKGK4UqV4LFDEtbdYq1FuVGh2Q0TC4Z3TmyyYg/J/z7fDrGaCrKWLB4YiwdI5+KamsRkWNKw5oi8nByzheYr6zA9jKUl4Netxu759INcF0/v230DIye9q9HZvxChswUpCd9D90RBbhez1FpdSjV2hRrbcqNNuW6P5fq/tiqtSnW/DYjxVrbn+ttAPLJKPmUD3Hj6ThT2d194aZGEkxl48zmkqrOIPII0bCmiDyczCA97o/pp+98X30Ltq753ratBb8p78Kf+iHVyipUCuB6vqctP++HUfOn/Dl7IuiZm4Z49p4CXChkZBO+l2z+gOsfGu0uxVo/wLXYqLQobDdY227y6sIWa9tNVsu+OkOt1WUmm2Am6zf2HUvHGE3HGEtFGU3HyCWjpOMRUrEwqViEdCy8816hTuThoHAmIo+G5Kg/Tjx753uaFd/bVrruh1GLi3DlRd8bV1nx899cL+htm4bMJKSndnveRmZ2h1NTExA6nDlmiWiYmVyYmdzd59bVW76Y/c1SnY1Ki62gJ+7qepXNa1uU6m3qrS7VVpdaq0O12aXSbNPpumCo1R/j6RinxlKcnUhzZiLN2fE0J/IJIpo3JzJwGtYUEdmrue1LYFVWobrme9yqa35F6vYKbN/0Aa+5HcyBO+HDWyZYFJGZ9kEuloFYevccz9xzr9yh/FqdLuV6Z2eIdaPi95B7f73KwkaVhfUaq+UGsUiIeCREIhreOeeSUcYzMUZTMcaD0lzT2QQzuQSzuSSTI3GtZBW5BxrWFBHZj/iIPyYe/+j72o0gqC0Hw6Zr/nz9ZT+U2qpCqxKcq9As+41+s7M+0GVndxc57PTUTd/X0OpH/lqRMJMjYSZH7lzpodPt0ej4LUeanR6Ndpd6u0up7ufH9Y+r61VeurrBSqnhS3bVWkxk4oymYowkImSTUX9ORHdKcmWT/v1IIkoqHiYaChEOGdGwEQmHSMfDTGbiGnoVQeFMROTeRBNBtYTH9v81zUqwqOHm7rm4CEuv7PbWVVZ9iBuZ2T0yM7tDqzvHuB9ajaUPLchFwiEy4RCZ+MH+amh3e6yWGzsLIbYbHcr1drAVSYeVcoP3VoPrjTbVVpdur0en6+j0HJ1uj1K9Tbfndqo8PD6V4dRYCoCe8/d1e36kZ3Ikzsl8kplcgnhEK13l0aNwJiLyoMQzED8PE+c/+r5mxYe07eVgKHUl2Cvusj9XC364tbbhV6mmxiE55isxpMYhPeEDXGo8CHITPsilxvx94cP9oz8aDgVVF+7v+2xUmlxeq3BpzdddfenKBmZGOASRoKet53xt1pvFOmvlJtlkhNlckmwysrMAIhX35+lsgvmxFPOjKebHkoyoHqs8JBTORESGTTzjj/Fzd7+3VYP6pt9upL4J1XUf2qoFWL3o95CrrvvPaxt+VWs844PbTqgb3w12e4/0hD8nchA6+h6q8Uyc8UycP/fY+L7u7/Uc65UmN0sNKsFGwbuLIDosbdV56coGi1s1FjfrxKMhJjNxvygiGH7NJvzmwT3n6DnfO9dzEAkZmX6FiLgfms2nopwaS3FyNKkeOzlSCmciIg+zWMofubn93d/rQaPog1o/0NU2do/NK1Dd87627hc/xLO7K2KTo0Ev3Ohuj11ybM/neX8+4lAXChlT2QRT2buvcnXOsRHMmevvP9dfINHu9giZEQ4ZIfPft9N1VJr+8xtbdbYbHTarLRa3aiwXG0xkYsyPpTiRTxILh4iEjWg4RCTk59CZgUFwNkIhYyITLKQIFlNMZLSQQm5P4UxE5DgJhYJesgNsxtbrQqPke936ga6+tft+/T1/bhR3r9eLPtTFMkFYy0Mivxvs0lN+q5LMtH+dnvCfJ3KHPuwKYGZMZOJMZO68IGK/Ot0ey6UGi5s1lksN2t0e7WDuXLvbo931c+OcczgHDuj0HO+ubPO99wqslBuslJqU6i1yySijqf5edf6cjIaJRoxoaDf05VNR5kZTnMwnmRtNqqrEI05baYiIyNHodf0q1X5Y64e32qZf3bp3q5JqwQfARhmiyd2g1u+h6/fOpcaCkJf/8DmWGdhWJfei3e2xVWuxVfWrYft71jXaXVrdYMFEt0er6yjWWixt1VnaqnGz1CCbiDKT8ytk8ylfFiyfipEPVsqOJCKk4xEycf86GYuQioZJxcPEwiGtih0S2kpDREQerFB4d6hzv5zzPW79nrr61gfn1JWXYe1tH/bqW7uBr1GCbssHup3QlvtwgLOwv6/X8edu22+dMnYWRs/680Ge9z5EwyFfmmvk7sOye/XrvK4E25gUa76yxFatzcJGlUqzQ7XZodLsUGl02G52qLe61IKNiXsOUtEw8WiYRPSDe9oloiHikfCH9rrzFSfCwesIM7k4Z8bTzI+liGrj4kOncCYiIsPDDBJZf+TnD/a1nWYQ6oJeug+8LvreOuf8sGk4BqGo76VrlODi78LW+7C54D/PzX2wOkRmyoe9XtevkO31fDWJUNiXABs97cuBxdJH8p9lr1DImM4mmN7HXLvbaXd71Fpdmu0ujXaPRqdLc+95zz53jY6/p97qUG932ai2qDW7fOdig4X1KivlBidyCc5OpJkaSZCKh8nEg5WzcR/kMkEPXiYRIRMPE4+E6faCbVSCLVXMYDJYEKJ5eApnIiLyqIjEg2oNU/f+PZzzCyFKix8cci0vw9o7fs6ehX0o6/fCvfcdX9e1tOh74fKnglWu+Q8ukoiPfLBqRCy927sXzx5aObC7iYZD5JIhSN7/1iLNTpfFzTrvr1fZqDSpNDvUWl2K9RY3irsrZ3d68podmu0ekbBfgBEJGZGQXy1b2G5SqrcZS8eYysaZGkkwkYkxEYS2iUyM8XScdNzXi01Gd+vGPmpz8BTORERE+syCfeImDv61vZ7fn660GAzD7hl23bzq969rV/dUjagE8+yK/n0ie+f5dLfb8iSehUgMwvGgJ/DBDy/GI2EeDzYNPgztbo/1SpPVcpO1coONaouNSpPFzRqvLRbZqDSpBgGw1upSbfngFwuHmM4mmByJM51NMBWcp3PB6thsgqls/KEJcQpnIiIihyEU8mW5srMH/9puZ3eeXaP4wQUUjaLfiHjtrT1bnGz6xRadFnSbvgcvFPW9cf2SYP0KE+kpH/z6pcni2d1zIjtUCymi4RCzuSSzueS+v8Y5R7nRobDd8KEuON8s1fnB4harZT8/r7DdJBYJ7SyYGEn4xRPxSGinWkU3GGq9MJfnK5978gh/04+mcCYiIjJo4YgvyZXe3wa8H+KcD2jNClSCqhKVVX8u34DC236hRf9olH24a5Sh0/Bhrd9zl8h9eBVsNOXn58XS/pwc9VUn+hsVhwdXfcHMyCWj5JJRHp8aueN9vZ6j0uoEZcXaO2c/zOq3LYmE/HDrWDr2AH+DD1M4ExERediZ+Tl3kbgPeNNP7/9ru+0gsN2yiKJ/btV80GvXoF33Q7CNoq88US343r1YZk+ZsGD4NT3hQ9zteuv6c+0OsTbs3YRCRjbhq0LA/nvmBkHhTERE5DgLRw++MfFevV6wf11QUaK2sVtGrLIGG1eCHrvybq9dP/z1Ors9df3FE/2FFImcD5vhWHBEIZLw4W9nA+NJf88jRuFMRERE7l0otGdI9omDfe3O9ie3bFS802tXhe6W793rNqHdCMJffwPjgi9f1g91idzucevmxf1asv3rAxyKvRuFMxERERmM+93+pF8rtr8R8c5R3O3NW7+8p4Zsv/xY0c+jS43uqQub3w15M8/AMz99uL/rASiciYiIyMPpXmrFgg91zVIQ1orQ2NqtOlEv+rl1A6RwJiIiIsdLKHTw0mIP0JHtWGdm3zCzNTN78w6fm5l91cwum9nrZvapPZ991szeDT778lE9o4iIiMiwOcrthH8d+OxHfP6TwPng+HngVwHMLAx8Lfj8KeALZvbUET6niIiIyNA4snDmnPsesPkRt3we+E3n/V8gb2azwPPAZefcVedcC/hWcK+IiIjII+/BF+LadRJY3PN+Kbh2p+siIiIij7xBhrPbbQnsPuL67b+J2c+b2atm9mqhUDi0hxMREREZhEGGsyVgfs/7OeDmR1y/Lefc151zzznnnpucnDySBxURERF5UAYZzr4N/K1g1eangZJzbhl4BThvZmfNLAa8ENwrIiIi8sg7sn3OzOybwGeACTNbAn4FiAI45/4j8AfA54DLQA34YvBZx8z+HvA/gTDwDefcxaN6ThEREZFhcmThzDn3hbt87oBfvMNnf4APbyIiIiLHyiCHNUVERETkFgpnIiIiIkNE4UxERERkiCiciYiIiAwRhTMRERGRIaJwJiIiIjJEFM5EREREhojCmYiIiMgQUTgTERERGSLmN+p/NJhZAbh2xD9mAlg/4p8hB6d2GV5qm+GkdhleapvhdBTtcto5N3nrxUcqnD0IZvaqc+65QT+HfJDaZXipbYaT2mV4qW2G04NsFw1rioiIiAwRhTMRERGRIaJwdnBfH/QDyG2pXYaX2mY4qV2Gl9pmOD2wdtGcMxEREZEhop4zERERkSGicLZPZvZZM3vXzC6b2ZcH/TzHmZnNm9mfmNnbZnbRzL4UXB8zsz8ys0vBeXTQz3ocmVnYzH5gZr8fvFe7DAEzy5vZb5vZO8H/O39ebTN4Zvb3gz/H3jSzb5pZQu0yGGb2DTNbM7M391y7Y1uY2VeCTPCumf2Vw3wWhbN9MLMw8DXgJ4GngC+Y2VODfapjrQP8A+fck8CngV8M2uPLwIvOufPAi8F7efC+BLy9573aZTj8O+A7zrmPAxfwbaS2GSAzOwn8EvCcc+4ZIAy8gNplUH4d+Owt127bFsHfOS8ATwdf8x+CrHAoFM7253ngsnPuqnOuBXwL+PyAn+nYcs4tO+f+X/B6G/+XzEl8m/xGcNtvAH99ME94fJnZHPBXgV/bc1ntMmBmlgX+EvCfAZxzLedcEbXNMIgASTOLACngJmqXgXDOfQ/YvOXyndri88C3nHNN59z7wGV8VjgUCmf7cxJY3PN+KbgmA2ZmZ4BPAi8D0865ZfABDpga3JMdW/8W+MdAb881tcvgPQYUgP8SDDn/mpmlUdsMlHPuBvCvgevAMlByzn0XtcswuVNbHGkuUDjbH7vNNS1zHTAzywD/Hfhl51x50M9z3JnZTwFrzrk/G/SzyIdEgE8Bv+qc+yRQRUNlAxfMX/o8cBY4AaTN7GcH+1SyT0eaCxTO9mcJmN/zfg7f9SwDYmZRfDD7Lefc7wSXV81sNvh8Flgb1PMdU38B+GtmtoAf+v9xM/uvqF2GwRKw5Jx7OXj/2/iwprYZrL8MvO+cKzjn2sDvAD+G2mWY3KktjjQXKJztzyvAeTM7a2Yx/CTAbw/4mY4tMzP83Jm3nXP/Zs9H3wZ+Lnj9c8DvPehnO86cc19xzs05587g/x/5Y+fcz6J2GTjn3AqwaGYfCy79BPAWaptBuw582sxSwZ9rP4GfQ6t2GR53aotvAy+YWdzMzgLnge8f1g/VJrT7ZGafw8+nCQPfcM79ywE/0rFlZn8R+FPgDXbnNv1T/Lyz/wacwv+h9zecc7dO7pQHwMw+A/xD59xPmdk4apeBM7Nn8Qs1YsBV4Iv4f6CrbQbIzP458DP4Veg/AP4ukEHt8sCZ2TeBzwATwCrwK8Dvcoe2MLN/BvxtfNv9snPuDw/tWRTORERERIaHhjVFREREhojCmYiIiMgQUTgTERERGSIKZyIiIiJDROFMREREZIgonInIsWBmXTN7bc9xaDvkm9kZM3vzsL6fiBxvkUE/gIjIA1J3zj076IcQEbkb9ZyJyLFmZgtm9q/M7PvB8Xhw/bSZvWhmrwfnU8H1aTP7H2b2w+D4seBbhc3sP5nZRTP7rpklB/ZLichDTeFMRI6L5C3Dmj+z57Oyc+554N/jK4EQvP5N59wngN8Cvhpc/yrwv5xzF/D1KS8G188DX3POPQ0UgZ8+4t9HRB5RqhAgIseCmVWcc5nbXF8Aftw5d9XMosCKc27czNaBWedcO7i+7JybMLMCMOeca+75HmeAP3LOnQ/e/xMg6pz7F0f/m4nIo0Y9ZyIi4O7w+k733E5zz+sumtMrIvdI4UxExBee7p9fCl7/H+CF4PXfBP538PpF4BcAzCxsZtkH9ZAicjzoX3Yiclwkzey1Pe+/45zrb6cRN7OX8f9g/UJw7ZeAb5jZPwIKwBeD618Cvm5mfwffQ/YLwPKRP72IHBuacyYix1ow5+w559z6oJ9FRAQ0rCkiIiIyVNRzJiIiIjJE1HMmIiIiMkQUzkRERESGiMKZiIiIyBBROBMREREZIgpnIiIiIkNE4UxERERkiPx/IJXDvYeco2QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learningCurve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
